{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Written By: kerrie geil\n",
    "Original Development Date: September 2024\n",
    "Package Requirements: xarray, rioxarray, netcdf4, geopandas, shapely, dask, distributed, matplotlib, glob2\n",
    "Description: data prep for NOAA nClimGrid-Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client,LocalCluster\n",
    "import xarray as xr\n",
    "import dask\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import rioxarray as rio\n",
    "import shapely\n",
    "import os\n",
    "os.environ['GDAL_DATA'] = os.environ['CONDA_PREFIX'] + r'/Library/share/gdal'\n",
    "os.environ['PROJ_LIB'] = os.environ['CONDA_PREFIX'] + r'/Library/share'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir=r'C://Users/kerrie/Documents/02_LocalData/nclimgrid_daily/'\n",
    "# shpfile=r'C://Users/kerrie/Documents/02_LocalData/boundaries/study_area_bounding_box.shp'\n",
    "data_dir=r'E://data/nclimgrid_daily/'\n",
    "shpfile=r'E://data/boundaries/study_area_bounding_box.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may require adjustment on other computers\n",
    "\n",
    "# kerrie laptop client (machine has 32GB RAM and 20 threads)\n",
    "# nworkers=20  # set equal to number of threads you have\n",
    "\n",
    "# kerrie desktop client (machine has 64GB RAM and 16 threads)\n",
    "nworkers=16  # set equal to number of threads you have\n",
    "\n",
    "cluster=LocalCluster(n_workers=nworkers,threads_per_worker=1)\n",
    "client=Client(cluster) \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "1) drop any variables we don't need\n",
    "2) dim names are time, lat, lon\n",
    "3) time is correct datetimes\n",
    "4) lat is ascending\n",
    "5) lon is ascending -180 to 180\n",
    "6) variable names tmax,tmin,prcp\n",
    "7) units of C for temperature\n",
    "8) units of mm/day for precip\n",
    "9) spatial subset\n",
    "11) round all values to 2 decimal places\n",
    "12) save data with smallest possible precision\n",
    "13) write cleaned files, 1 file per variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=sorted(glob.glob(data_dir+'orig/ncdd*-grd-scaled.nc'))\n",
    "len(files), files[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy dataset, all 800+ files\n",
    "ds=xr.open_mfdataset(files,chunks='auto',lock=False)#.chunk({'time':-1,'lat':-1,'lon':25})\n",
    "# ds=xr.open_mfdataset(files,chunks={'time':-1,'lat':-1,'lon':25},lock=False)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) drop data we don't need\"\n",
    "ds = ds.drop_vars('tavg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps 2-7 aren't necessary, data already looks good wrt to these items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 8 millimeter --> mm/day\n",
    "ds.prcp.attrs['units']='mm/day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 9 spatial subset\n",
    "# clip data to a bounding box\n",
    "\n",
    "# get clip object\n",
    "box=gpd.read_file(shpfile)\n",
    "\n",
    "# assign crs to netcdf data\n",
    "ds.rio.write_crs(\"epsg:4326\",inplace=True)\n",
    "ds_clip=ds.rio.clip(box.geometry.apply(shapely.geometry.mapping),box.crs,drop=True,invert=False)\n",
    "ds_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata separately for later\n",
    "coords = ds_clip.coords\n",
    "prcp_attrs = ds_clip.prcp.attrs\n",
    "tmax_attrs = ds_clip.tmax.attrs\n",
    "tmin_attrs = ds_clip.tmin.attrs\n",
    "dims = ds_clip.dims\n",
    "print(dims)\n",
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 10 & 11, round and reduce precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a single time to test data precision, looking for values on order of at least 100\n",
    "testtime='2000-06-4'\n",
    "ds_clip.prcp.sel(time=testtime).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float16 is probably not enough precision, let's check on a subset of the data\n",
    "\n",
    "print('loading float16')\n",
    "prcp_16 = ds_clip.prcp.sel(time=testtime).astype('float16').load()\n",
    "print('loading float32')\n",
    "prcp_32 = ds_clip.prcp.sel(time=testtime).astype('float32').load()\n",
    "print('loading float64')\n",
    "prcp_64 = ds_clip.prcp.sel(time=testtime).load()\n",
    "\n",
    "prcp_16.max().item(),prcp_32.max().item(),prcp_64.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can change the data type from float64 to float32 but not go any smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps 10 & 11\n",
    "ds = ds_clip.round(decimals=2)\n",
    "ds = ds_clip.astype('float32')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write files\n",
    "\n",
    "I think what we have to do to write each chunk to a separate file is:\n",
    "- chunk xr arrays in space instead of time\n",
    "- convert to numpy so we can use to_delayed and ravel\n",
    "- every worker needs the xr metadata for variable and coordinates\n",
    "- write a dask delayed function that takes the numpy array data and the metadata separately\n",
    "- inside the dask delayed function re-create the xarray object and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking along longitude only\n",
    "ds = ds.chunk({'time':-1,'lat':-1,'lon':12})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to return a list of data chunks \n",
    "# and the corresponding longitude coord chunks\n",
    "def xr_ds_to_delayed(ds,varname):\n",
    "    # chunk the appropriate variable in ds, delay, ravel to list\n",
    "    var_chunks = ds[varname].data.to_delayed().ravel()\n",
    "    # xarray doesn't allow chunking of coordinates, so we have to make a new variable to chunk\n",
    "    lon_chunks = xr.DataArray(ds.lon.data,coords={'lon':('lon',ds.lon.data)}).chunk({'lon':12}).data.to_delayed().ravel()\n",
    "    return var_chunks,lon_chunks\n",
    "\n",
    "# numpy back to xarray, reattaching metadata and writing chunks to separate files\n",
    "def write_chunk_to_netcdf(datapath,chunk_id,varname,np_datachunk,xr_time,xr_lat,np_lonchunk,lon_meta,var_meta):\n",
    "    xr_datachunk = xr.Dataset({varname:(['time','lat','lon'],np_datachunk)},coords={'time':('time',xr_time.data),\n",
    "                                                            'lat':('lat',xr_lat.data),\n",
    "                                                            'lon':('lon',np_lonchunk)})\n",
    "    # copy over xr metadata\n",
    "    xr_datachunk.time.attrs=xr_time.attrs\n",
    "    xr_datachunk.lat.attrs=xr_lat.attrs\n",
    "    xr_datachunk.lon.attrs=lon_meta\n",
    "    xr_datachunk[varname].attrs=var_meta\n",
    "\n",
    "    # clean up metadata\n",
    "    attrslist=['time','lat','lon',varname]\n",
    "    for att in attrslist:\n",
    "        if 'valid_min' in xr_datachunk[att].attrs:\n",
    "            del xr_datachunk[att].attrs['valid_min']\n",
    "        if 'valid_max' in xr_datachunk[att].attrs:\n",
    "            del xr_datachunk[att].attrs['valid_max']\n",
    "        if (att!=varname) and ('comment' in xr_datachunk[att].attrs):\n",
    "            del xr_datachunk[att].attrs['comment']\n",
    "        if 'id' in xr_datachunk[att].attrs:\n",
    "            del xr_datachunk[att].attrs['id']            \n",
    "    \n",
    "    # write file\n",
    "    xr_datachunk.to_netcdf(datapath+varname+'_nClimGridDaily_USsouth_'+str(chunk_id).zfill(2)+'.nc')\n",
    "    return str(chunk_id).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "var_chunks,lon_chunks = xr_ds_to_delayed(ds,'prcp')\n",
    "task_list= [dask.delayed(write_chunk_to_netcdf)(data_dir,id,'prcp',\n",
    "                                                datachunk,ds.time,ds.lat,\n",
    "                                                lonchunk,ds.lon.attrs,ds.prcp.attrs) \\\n",
    "            for id,(datachunk,lonchunk) in enumerate(zip(var_chunks,lon_chunks))]\n",
    "completed_files = dask.compute(*task_list)\n",
    "completed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "var='tmax'\n",
    "var_chunks,lon_chunks = xr_ds_to_delayed(ds,var)\n",
    "task_list= [dask.delayed(write_chunk_to_netcdf)(data_dir,id,var,\n",
    "                                                datachunk,ds.time,ds.lat,\n",
    "                                                lonchunk,ds.lon.attrs,ds.prcp.attrs) \\\n",
    "            for id,(datachunk,lonchunk) in enumerate(zip(var_chunks,lon_chunks))]\n",
    "completed_files = dask.compute(*task_list)\n",
    "completed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "var='tmax'\n",
    "var_chunks,lon_chunks = xr_ds_to_delayed(ds,var)\n",
    "task_list= [dask.delayed(write_chunk_to_netcdf)(data_dir,id,var,\n",
    "                                                datachunk,ds.time,ds.lat,\n",
    "                                                lonchunk,ds.lon.attrs,ds.prcp.attrs) \\\n",
    "            for id,(datachunk,lonchunk) in enumerate(zip(var_chunks,lon_chunks))]\n",
    "completed_files = dask.compute(*task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var='prcp'\n",
    "files = glob.glob(data_dir+var+'_nClimGridDaily_USsouth_*.nc')\n",
    "test=xr.open_mfdataset(files)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[var].isel(time=15).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old code below to write 1 single file per variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename='prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "print('writing',filename)\n",
    "ds.prcp.to_netcdf(data_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename='tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "print('writing',filename)\n",
    "ds.tmax.to_netcdf(data_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename='tmin_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "print('writing',filename)\n",
    "ds.tmin.to_netcdf(data_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=xr.open_mfdataset(data_dir+filename)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerr",
   "language": "python",
   "name": "nerr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
