{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c84c157e-82c5-4028-9cb9-4084f231216c",
   "metadata": {},
   "source": [
    "Written By: kerrie geil\n",
    "Original Development Date: September 2024\n",
    "Package Requirements: xarray, numpy, pandas, bottleneck, matplotlib\n",
    "Description: Coding up KBDI for the first time using obs. The plan is to create a function out of this and put it in a .py script that I can load into other notebooks/scripts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69503410-2e59-4c16-a92f-47544cf6748b",
   "metadata": {},
   "source": [
    "keep this cell type as raw or the text gets all kinds of messed up\n",
    "\n",
    "SOURCE: https://wikifire.wsl.ch/tiki-index908f.html?page=Keetch-Byram+drought+index\n",
    "\n",
    "The KBDI  is calculated as follows:\n",
    "\r\n",
    "KBDI of t = Q + ( (800 - Q) * (0.968 * e^(0.048 6 *T) - 8.30) - 8.30) * Δt ) / ( 1 + 10.88 * e^(-0.0441 * P) ) * 10^(-3\n",
    "\n",
    "Q = (KBDI of t−1) − (Pnet of t) * 100\n",
    "\n",
    "Pnet of t = max( 0, (P of t) - max(0, Plim - ∑ from i=1 to rr-1 of P of t−i) ) >\n",
    "−where \n",
    "Q is the previous day's KBDI minus net rainfall (in/100) (see below), \n",
    "T is air temperature (°F) (maximum air temperature or the dry-bulb temperature at time of basic observation), \n",
    "Δt is time increment (one day),\n",
    "P is mean annual precipitation (in)rr is the number of consecutive \n",
    "days with rain.\r\n",
    "\r\n",
    "\r\n",
    "In order to obtain net rainfall, 0.2 inch has to be subtracted from any daily rainfall amount exceeding 0.2 inch. In the case of consecutive days with rainfall, the 0.2 inch has to be subtracted on the exact day when the summed rainfall amount exceeds 0.2 inch. If the daily rainfall amount is smaller than 0.2, then net rainfall equals zero. Finally, net rainfall, expressed n hsundrsedt an ofrainchs, h a tobbtra su cfred  m Q\r\n",
    "\n",
    "\r\n",
    "Th*  KBDI\r\n",
    " is cumulative, i.e. it is imperative to start the index computation when upper soil layer are saturated with water, e.g. after a period of abundant rainfall, such as 6 or 8 inches iodn a peri of a week. Thfirst **index value is then set to 0.\r\n",
    "\r\n",
    "NB: There is a typ eographicalrror in the orinal pap**er of Keetch and Byrntm (1968). The lntst ctonsta  in he numerator o the KBDI\r\n",
    " equation should have been 8.3 (as indicated here) and not 0.830 (as ndicated (riginal p (ion; cf.*2* CraAne 198 and lexander 1990) \n",
    "\r\n",
    "The KBDI\r\n",
    " is supposed to be calculated on a daily basis. No specific time for the recording of meteorological data is stipulated. Thus the meteorological data used for its calculaeteorologicaltare the m cal da ime\n",
    "<bsr><br> of the bbssicatwoether o erv in .\r",
    "\r\n",
    "A funct in mmofa th KBDI\r\n",
    "  ] hs tdo be idedin dthe  F\n",
    "\n",
    "**FDI\r\n",
    " an theSharples index frmulas. The KBDI\r\n",
    " in S.I. unitsure with air temperatrend prciitaioexprese 1990)espectively, islows (Crane 1982 and Alexander 1990): alculated as fol**\n",
    "\n",
    "KBDI of t = Qsi + ( (203.2 - Qsi) * (0.968 * e^(0.0875*T+1.5552) - 8.30) * Δt) / (1 + 10.88 * e^(- 0.001736)\n",
    ") ) *10^(-3) <br>\n",
    "Qsi = (KBDI of t* 100−\n",
    " (Pnet of t) <br>\n",
    "Pnet of t = max( 0, (P of t) - maxlim to rr-1 - ∑ over rr-1 \n",
    "\n",
    "f P of t−i)) <br>of consecutive days with rains\n",
    "\r\n",
    "QSI=KBDISIt−1−Pnett\r\n",
    "\r\n",
    "\r\n",
    "It is also imperative to start the index computation when upper soil layer are saturated with water, e.g. afaiter a period of abundant rnfall, such as 2 or 203 mm in a period **of a week. The first index value is then set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4520e6f-87d2-48a2-b74d-1f8acdca7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ce234-9976-4fd1-9035-a4df6b1f94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "\n",
    "pr_file = r'E://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "tmax_file = r'E://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "\n",
    "\n",
    "\n",
    "# chunks = {'time':-1,'lat':50,'lon':50}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f6666-4a2b-4920-b288-b578f45cbe5f",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) find the starting day of the calculation at each grid cell (KBDI initialization)\n",
    "    - when the running sum of precip over a 1 week period is greater than 152mm (or 203?)\n",
    "    - the starting day for calculating KBDI is the next day (after the week that qualifies above) \n",
    "2) calculate rr for the whole timeseries at each grid (the number of consecutive days with rain)\n",
    "3) calculate Pnet of t\n",
    "4) calculate Q of t0 using KBDI of t-1 = 0\n",
    "5) calculate KBDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c1cf6-17cd-4328-a2b3-902f3f172cf7",
   "metadata": {},
   "source": [
    "# 1) KBDI Initialization date at each grid cell\n",
    "\n",
    "Finding the first possible day at each grid where we can set the initial value of KBDI to zero. This requires a preceding wet period where at least 152mm of precip falls with 1 week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63544bf3-ff78-42e0-95d1-3eb5728c4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=xr.open_dataset(pr_file).prcp.sel(time=slice('1951','1960'),lat=slice(30,32),lon=slice(-90,-88)).round(2).load()\n",
    "# pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960')).round(2)#.load()\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1f758-e45c-4d4d-ac1f-90298a1fe56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a land/sea 1/0 mask (to help us keep track of where the nans should be)\n",
    "landmask=xr.where(np.isfinite(pr.mean('time')),1,0)\n",
    "nlandpts = landmask.sum().data\n",
    "print(f'{nlandpts} data points (grids) over land')\n",
    "landmask.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2c571-82a6-4083-aaa6-59c3da900897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an integer time index since datetimes will give us some trouble\n",
    "time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "pr.coords['time_index']=('time',time_index)\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bb3d6-cceb-48bf-a3c3-ea3d4e0a66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum precip in 7 day rolling windows\n",
    "ndays=7\n",
    "pr_thresh=152\n",
    "pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "pr_weeksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccc1cd-8ca4-45ae-b3b8-4c18cb7ced01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the max 7 day sum\n",
    "pr_weeksum.max('time').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5638e-6d4c-4e6a-b335-a4b68cf4c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first index time where the weekly sum meets the threshold\n",
    "day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "day_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61678c-0349-4e5d-8832-4cd2ce66a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(day_index.min().data, day_index.max().data)\n",
    "\n",
    "assert day_index.min().data >= 7,f'found intialization at index {day_index.min().data.item()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a8077-7814-4a32-82d7-091b54325ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the result\n",
    "day_index.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455c947-458b-4fa9-a0f9-3f3c9c8110af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any grids where we didn't find a time index (more nans than the landmask)\n",
    "\n",
    "nnan_mask=landmask.sum().data # nans in the landmask (ocean points)\n",
    "nnan_data=xr.where(np.isfinite(day_index),1,0).sum().data # nan grids in our initialization index (should be only ocean points)\n",
    "\n",
    "assert nnan_mask == nnan_data, f'there are {nnan_data - nnan_mask} grid cells where a KBDI initialization index was not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f37759-9063-4c8e-b4a9-f8785cb1a7dd",
   "metadata": {},
   "source": [
    "figure out which day of the week/window is given in our index, is it the first day or last day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96fdba-950c-44d0-acd0-de9ae8f87d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at rolling pr and plot the threshold as well as the date where the threshold is met for a single grid\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=pr_thresh,color='grey',linestyle='dashed',linewidth=0.5)  # threshold guide line\n",
    "plt.axvline(x=pr_weeksum.time[day_index.isel(lat=0,lon=0).astype('int').item()].data,color='red')\n",
    "pr_weeksum.isel(lat=0,lon=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf376826-b525-4729-8a9c-849f204caf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=pr_thresh,color='grey',linestyle='dashed',linewidth=0.5)  # threshold guide line\n",
    "plt.axvline(x=pr_weeksum.time[day_index.isel(lat=0,lon=0).astype('int').item()].data,color='red')\n",
    "pr_weeksum.isel(lat=0,lon=0).sel(time=slice('1952-07-05','1952-07-19')).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f43b5-1472-4bcd-bc01-918ec2002966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a little test of rolling on this grid for these dates\n",
    "test=pr.isel(lat=0,lon=0).sel(time=slice('1952-07-05','1952-07-19')).rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39447ca5-7ad0-401d-8ebd-a56a4574f6da",
   "metadata": {},
   "source": [
    "Because we chose min_periods=4 and the first 3 values are nan we can see that .rolling with center=False returns the sum of the 7 day window with the time label on the last day in the window. So the sum is of that day plus the preceding 6 days\n",
    "\n",
    "**Therefore, this index/date is the KBDI initialization time t-1. The first KBDI will be computed one day after this at time t.**\n",
    "\n",
    "# 2) rr, number of consecutive days with rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ed48c-9e58-4d08-a3fa-704afa2680e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding it up for a single grid first\n",
    "prsub=pr.isel(lat=0,lon=0)\n",
    "prsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28189b56-d1b0-47ea-a51f-3150968f59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rainmask=xr.where(prsub>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b0a4b-45b5-4db9-93bf-ac87a3d72c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of consecutive rain days\n",
    "\n",
    "# I got this code to interrupt a cumulative sum here:\n",
    "# https://stackoverflow.com/questions/61753567/convert-cumsum-output-to-binary-array-in-xarray\n",
    "rr=rainmask.cumsum()-rainmask.cumsum().where(rainmask == 0).ffill(dim='time').fillna(0)\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72401405-e60d-4b70-9028-a12cd183fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if it's really working\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=0,color='grey')  # threshold guide line\n",
    "plt.ylim([-1,10])\n",
    "prsub.sel(time=slice('1951-01-01','1951-01-31')).plot()\n",
    "rr.sel(time=slice('1951-01-01','1951-01-31')).plot(marker='o',linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab667c-1860-4628-bd71-6db4b27f676e",
   "metadata": {},
   "source": [
    "wow, ok it is working. blue line is daily precip, orange dots are the number of consecutive rain days.\n",
    "\n",
    "Now do this to the whole array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b499ed-89f5-4eba-865e-2be786491524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing, whole array\n",
    "rainmask=xr.where(pr>0,1,0)\n",
    "rr=rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)\n",
    "print(rr.shape)\n",
    "\n",
    "# plot should be identical\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=0,color='grey')  # threshold guide line\n",
    "plt.ylim([-1,10])\n",
    "pr.isel(lat=0,lon=0).sel(time=slice('1951-01-01','1951-01-31')).plot()\n",
    "rr.isel(lat=0,lon=0).sel(time=slice('1951-01-01','1951-01-31')).plot(marker='o',linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccacf1b4-048a-4a13-87a8-267749c6bc1d",
   "metadata": {},
   "source": [
    "plots for the test grid are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480abeb-5709-4b06-8ba5-9cdb2477c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a single time in space\n",
    "rr.sel(time='1951-01-15').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6846e-00e0-433a-88f9-c523c707d78a",
   "metadata": {},
   "source": [
    "we need to put the nans back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2721d6-39f2-4fa1-abc3-df8c0d37e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put nans back\n",
    "# .where reorders dims so we need .transpose to get original dim order back\n",
    "rr=xr.where(landmask,rr,np.nan).transpose('time','lat','lon') \n",
    "# rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62426a-4659-46e4-8eaf-215bbc9792f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a single time in space\n",
    "rr.sel(time='1951-01-15').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70a312-3071-4e9d-b5cc-4ac7cadd281d",
   "metadata": {},
   "source": [
    "looks good (note the different scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020602de-52ce-4442-b11a-25ee3b10363d",
   "metadata": {},
   "source": [
    "# 3) Pnet\n",
    "\n",
    "In order to obtain net rainfall, 0.2 inch has to be subtracted from any daily rainfall amount exceeding 0.2 inch. In the case of consecutive days with rainfall, the 0.2 inch has to be subtracted on the exact day when the summed rainfall amount exceeds 0.2 inch. If the daily rainfall amount is smaller than 0.2, then net rainfall equals zero. Finally, net rainfall, expressed in hundredths of an inch, has to be subtracted from Q.\n",
    "\n",
    "Pnet of t = max( 0, (P of t) - max(0, Plim - ∑ over i=1 to rr-1 of P of t−i)) \n",
    "\n",
    "I have no idea what this equation means, it doesn't seem to match the text description of how to calculate Pnet at all. Also, it's never explained what Plim is.\n",
    "\n",
    "Approach:\n",
    "\n",
    "- split the timeseries into categories (modify rr into the following categories)\n",
    "    - category 0: days not in a consecutive rain event\n",
    "        - days where no rain falls\n",
    "        - days where less than 0.2 inch (5.08 mm) falls but is not part of a consecutive day rain event\n",
    "        - days where more than 0.2 inch (5.08 mm) falls but is not part of a consecutive day rain event\n",
    "    - category 1: days in a consecutive day rain event\n",
    "        - this means any day that is in a consecutive day rain event regardless of over/under threshold\n",
    "        - we'll have to loop through each consecutive rain event and\n",
    "            - accumulate rainfall totals over consecutive days\n",
    "            - rain amounts (lt 5.08 mm) on days before threshold is met should be set to 0\n",
    "            - subtract 0.2 inches (5.08 mm) only from the exact day when threshold is met\n",
    "            - rain amounts on days after threshold is met should remain unchanged and add to the accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a364362-ac0c-4b71-ab61-4a6c2ea86988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates two categories of rain days\n",
    "# cat 0 is a day that is not part of a consecutive-day rainfall event\n",
    "# cat 1 is a day that IS part of a consecutive-day rainfall event\n",
    "\n",
    "def create_categories(rr_chunk,chunk_id):\n",
    "    # for all consecutive rain events \n",
    "    # replace the third or greater day's value with an integer that is not 0, 1, or 2, we'll use 5\n",
    "    cat_chunk=xr.where(rr_chunk>=3,5,rr_chunk)\n",
    "\n",
    "    # we now need to find whether day 1's should be cat0 or cat1\n",
    "    # we'll know the answer based on whether the following day is a 2\n",
    "    \n",
    "    # find all indexes of the second day of consecutive rain events\n",
    "    day2_iii=np.argwhere(rr_chunk==2)\n",
    "\n",
    "    # loop through the indexes found above and replace those \n",
    "    # day's values as well as the previous day's value with 5\n",
    "    for i in range(day2_iii.shape[0]):\n",
    "        # replace day 2's value with 5\n",
    "        cat_chunk[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "        # replace day 1's value with 5\n",
    "        cat_chunk[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "\n",
    "    # days in a consecutive day rain event are 1\n",
    "    # days not in a consecutive day rain event are 0\n",
    "    # nan exists for non-data grids (over ocean cells)\n",
    "    cat_chunk=xr.where(cat_chunk==1,0,cat_chunk)\n",
    "    cat_chunk=xr.where(cat_chunk==5,1,cat_chunk)\n",
    "\n",
    "    # make sure there are not more than 3 different values (0,1,nan) present\n",
    "    assert len(np.unique(cat_chunk))<=3,f'too many category values {np.unique(cat_chunk)} in chunk id {chunk_id}' \n",
    "\n",
    "    return cat_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197380e-fafc-408f-a08a-2ee02430c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk over longitude for parallel processing\n",
    "rr=rr.chunk({'time':-1,'lat':-1,'lon':3})\n",
    "\n",
    "# delay the data chunks and get them into a 1D list\n",
    "rr_delayed=rr.data.to_delayed().ravel()\n",
    "len(rr_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f39ab-3920-431e-a27c-5561575c5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do the calculation\n",
    "\n",
    "# list of delayed compute tasks (1 for each chunk)\n",
    "task_list=[dask.delayed(create_categories)(rr_chunk,id) for id,rr_chunk in enumerate(rr_delayed)]\n",
    "\n",
    "# compute in parallel\n",
    "result_chunks=dask.compute(*task_list)\n",
    "\n",
    "# concatenate results\n",
    "cat_np=np.concatenate(result_chunks,axis=2)\n",
    "\n",
    "# numpy to xarray\n",
    "cat=xr.DataArray(cat_np,coords=rr.coords)\n",
    "cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b2e35-8247-4557-b2fe-04700e0cb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.sel(time='1951-01-15').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fb820-5480-4eef-9f83-142e98804dd2",
   "metadata": {},
   "source": [
    "We now have an array where every time at every grid falls into either cat 1 (day in consecutive rain event), cat 0 (day not in a consecutive rain event), or is nan. \n",
    "\n",
    "Next we have to loop in time to find the daily Pnet. Testing first on a single grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f5afb-0de2-44fd-9f99-da0914a5d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to single grid\n",
    "rr_sub=rr[:,0,0].compute()\n",
    "cat_sub=cat[:,0,0]\n",
    "pr_sub = pr[:,0,0]\n",
    "rr_sub.shape,cat_sub.shape,pr_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4001a-b389-45dc-90ef-f77034447df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize to nan\n",
    "Pnet = rr_sub.copy()\n",
    "Pnet[:]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1986387-ddcb-495d-8310-757eb280d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pnet condition for days not in consecutive rain event\n",
    "# subtract 5.08mm from precipitation amount\n",
    "# and if negative, replace with 0\n",
    "thresh = 5.08\n",
    "Pnet = xr.where(cat_sub==0, pr_sub-thresh, Pnet)\n",
    "Pnet = xr.where(Pnet<0,0,Pnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416688c9-8379-4a17-a3f7-e2c633e0cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now loop through consecutive rain events to find cumulative pr\n",
    "\n",
    "\n",
    "# initializations\n",
    "thresh_flag=False\n",
    "accpr=0.\n",
    "\n",
    "for i,(catval,prval) in enumerate(zip(cat_sub,pr_sub)):\n",
    "\n",
    "    # set/reset accpr to 0 and threshold flag when not raining consec days\n",
    "    if catval==0:\n",
    "        accpr=0.\n",
    "        thresh_flag=False\n",
    "\n",
    "    # if consec day rain\n",
    "    # subtract threshold where appropriate\n",
    "    # and accumlate rainfall over the event    \n",
    "    if catval==1:\n",
    "        accpr=accpr+prval # accumulated precip\n",
    "        # if not over the threshold yet, Pnet is 0\n",
    "        if accpr<thresh and not thresh_flag:\n",
    "            Pnet[i]=0\n",
    "        # on the day the threshold is met, subtract the threshold amount and change flag    \n",
    "        elif accpr>=thresh and not thresh_flag:\n",
    "            accpr=accpr-thresh # modify accumulated precip\n",
    "            Pnet[i]=accpr\n",
    "            thresh_flag=True\n",
    "        # any days after the threshold is met will accumulate with no limitations or subtractions   \n",
    "        else:\n",
    "            Pnet[i]=accpr            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4aa63-9047-4809-96c2-1e6764a55bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot should be identical\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=5.08,color='grey')  # threshold guide line\n",
    "plt.ylim([-1,10])\n",
    "Pnet.sel(time=slice('1951-01-01','1951-01-31')).plot()  # blue line\n",
    "pr_sub.sel(time=slice('1951-01-01','1951-01-31')).plot(marker='o',linewidth=0)  # orange circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9223da-92b2-4c2e-86a6-0d10c1bebcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet.min().item(),Pnet.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92c2be-c277-42f7-9643-c25a2b3b26a0",
   "metadata": {},
   "source": [
    "Looks like it's working correctly. Now apply to whole array. This is a time loop that will have to be applied to every grid cell. We'll send 3D chunks of data to the function and loop through the timeseries for each grid cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e263a-9ba1-4f81-b027-02da65c3c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the 3D array Pnet\n",
    "\n",
    "def calc_pnet(cat_arr,pr_arr):\n",
    "    # initializations and constants\n",
    "    thresh = 5.08 # mm\n",
    "    thresh_flag=False\n",
    "    accpr=0.\n",
    "    \n",
    "    # calc pnet where not consecutive day rain\n",
    "    pnet = np.where(cat_arr==0, pr_arr-thresh, np.nan)\n",
    "    pnet = np.where(pnet<0,0,pnet)\n",
    "\n",
    "    # loops required for days in consec day rain\n",
    "    for ilat in range(cat_arr.shape[1]):\n",
    "        for ilon in range(cat_arr.shape[2]):\n",
    "            for itime,(cat,pr) in enumerate(zip(cat_arr[:,ilat,ilon],pr_arr[:,ilat,ilon])):\n",
    "    \n",
    "                # set/reset accpr to 0 and threshold flag when not raining consec days\n",
    "                if cat==0:\n",
    "                    accpr=0.\n",
    "                    thresh_flag=False\n",
    "            \n",
    "                # if consec day rain\n",
    "                # subtract threshold where appropriate\n",
    "                # and accumlate rainfall over the event    \n",
    "                if cat==1:\n",
    "                    accpr=accpr+pr # accumulated precip\n",
    "                    # if not over the threshold yet, Pnet is 0\n",
    "                    if (accpr<thresh) and (not thresh_flag):\n",
    "                        pnet[itime,ilat,ilon]=0\n",
    "                    # on the day the threshold is met, subtract the threshold amount and change flag    \n",
    "                    elif (accpr>=thresh) and (not thresh_flag):\n",
    "                        accpr=accpr-thresh # modify accumulated precip\n",
    "                        pnet[itime,ilat,ilon]=accpr\n",
    "                        thresh_flag=True\n",
    "                    # any days after the threshold is met will accumulate with no limitations or subtractions   \n",
    "                    else:\n",
    "                        pnet[itime,ilat,ilon]=accpr  \n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4b685-e956-4993-b1d0-fd58085bfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk, delay, ravel\n",
    "chunk={'time':-1,'lat':-1,'lon':3}\n",
    "cat=cat.chunk(chunk)\n",
    "pr=pr.chunk(chunk)\n",
    "cat_delayed=cat.data.to_delayed().ravel()\n",
    "pr_delayed=pr.data.to_delayed().ravel()\n",
    "len(cat_delayed),len(pr_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044c5c8-f56a-437c-96ee-b27a7e6755d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do the calculation\n",
    "\n",
    "# list of delayed compute tasks (1 for each chunk)\n",
    "task_list=[dask.delayed(calc_pnet)(cat_chunk,pr_chunk) for cat_chunk,pr_chunk in zip(cat_delayed,pr_delayed)]\n",
    "\n",
    "# compute in parallel\n",
    "result_chunks=dask.compute(*task_list)\n",
    "\n",
    "# concatenate results\n",
    "pnet_np=np.concatenate(result_chunks,axis=2)\n",
    "\n",
    "# numpy to xarray\n",
    "Pnet=xr.DataArray(pnet_np,coords=pr.coords)\n",
    "Pnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a322f-2391-4c67-bea0-c0fee635885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results should be identical for the test grid we did earlier, let's check it\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=5.08,color='grey')  # threshold guide line\n",
    "plt.ylim([-1,10])\n",
    "Pnet.isel(lat=0,lon=0).sel(time=slice('1951-01-01','1951-01-31')).plot()  # blue line\n",
    "pr.isel(lat=0,lon=0).sel(time=slice('1951-01-01','1951-01-31')).plot(marker='o',linewidth=0)  # orange circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8050c-a82c-4dae-a4bd-9898c85218ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet.min().item(),Pnet.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905b2cc-3f4a-4d9e-acaa-acc22a658543",
   "metadata": {},
   "source": [
    "yes results are the same, everything working\n",
    "\n",
    "now we need to fill Pnet with nan for times before the KBDI initialization date at each grid cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61658f0-be2a-4267-8a06-8211862a605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_times(pnet_arr,init_ind,landmask):\n",
    "   \n",
    "    # get rid of nans and convert to integer\n",
    "    init_ind=xr.where(~np.isfinite(init_ind),0,init_ind).astype('int').data\n",
    "\n",
    "    # loop through grids and replace all values before the init index at each cell with nan\n",
    "    for ilat in range(pnet_arr.shape[1]):\n",
    "        for ilon in range(pnet_arr.shape[2]):\n",
    "            pnet_arr[0:init_ind[ilat,ilon]+1,ilat,ilon]=np.nan\n",
    "\n",
    "    # put any landmask nans back\n",
    "    pnet_arr=xr.where(landmask,pnet_arr,np.nan)\n",
    "    return pnet_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be187c-538a-40cb-99ec-25e3f20d51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pnet.shape,day_index.shape,landmask.shape)\n",
    "chunk2D = {'lat':-1,'lon':3}\n",
    "\n",
    "Pnet_delayed = Pnet.chunk(chunk).data.to_delayed().ravel()\n",
    "day_index_delayed = day_index.chunk(chunk2D).data.to_delayed().ravel()\n",
    "landmask_delayed = landmask.chunk(chunk2D).data.to_delayed().ravel()\n",
    "print(len(Pnet_delayed),len(day_index_delayed),len(landmask_delayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e744f-badd-4d44-afaa-ea13c20c2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do the calculation\n",
    "\n",
    "# list of delayed compute tasks (1 for each chunk)\n",
    "task_list=[dask.delayed(fill_nan_times)(pnet_chunk,dayind_chunk,mask_chunk) for pnet_chunk,dayind_chunk,mask_chunk in zip(Pnet_delayed,day_index_delayed,landmask_delayed)]\n",
    "\n",
    "# compute in parallel\n",
    "result_chunks=dask.compute(*task_list)\n",
    "\n",
    "# concatenate results\n",
    "pnet_np=np.concatenate(result_chunks,axis=2)\n",
    "\n",
    "# numpy to xarray\n",
    "Pnet=xr.DataArray(pnet_np,coords=pr.coords)\n",
    "Pnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e14af0-7f0d-49b7-9387-c20df4554022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results should show no blue line until one day after the initialization index\n",
    "# for grid 0,0 the init index is the 17th of July 1952 \n",
    "# so our blue line should start on Jul 18, 1952\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=5.08,color='grey')  # threshold guide line\n",
    "Pnet.isel(lat=0,lon=0).sel(time=slice('1951-01-01','1952-07-31')).plot()  # blue line\n",
    "pr.isel(lat=0,lon=0).sel(time=slice('1951-01-01','1952-07-31')).plot(marker='o',linewidth=0)  # orange circle\n",
    "\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y=5.08,color='grey')  # threshold guide line\n",
    "Pnet.isel(lat=0,lon=0).sel(time=slice('1952-07-15','1952-07-20')).plot()  # blue line\n",
    "pr.isel(lat=0,lon=0).sel(time=slice('1952-07-15','1952-07-20')).plot(marker='o',linewidth=0)  # orange circle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9c39f-2fd9-41ba-b326-f4b9235e3bb5",
   "metadata": {},
   "source": [
    "filling nan like this will allow us to apply the same KBDI calculation at each time from time 0 and the result will be nan at all grids before the KBDI initialization index\n",
    "\n",
    "# 4) load tmax data and calc mean annual precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f269296-ddfb-4101-8179-9237d7ae415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax=xr.open_dataset(tmax_file).tmax.sel(time=slice('1951','1960'),lat=slice(30,32),lon=slice(-90,-88)).round(2).load()\n",
    "tmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7600432-4c00-4981-b9b1-c9c301e0b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in range(0,10):\n",
    "#     pr.resample(time='YE').sum(min_count=10)[t,:,:].plot()\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1195b-fef0-4f92-afe6-901afb7a851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ann_pr = pr.resample(time='YE').sum(min_count=10).mean('time').compute()\n",
    "mean_ann_pr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4bae1-95b7-4a81-ba63-8c991d33e289",
   "metadata": {},
   "source": [
    "# 4) Calculate KBDI \n",
    "\n",
    "using SI units mm and C\n",
    "$$\n",
    "KBDI_{t} = Q_{t} + \\frac{(203.2 - Q_{t}) (0.968 e^{0.0875 T_{t} + 1.5552} - 8.30) Δt}{1 + 10.88 e^{-0.001736 P}} * 10^{-3}\n",
    "$$\n",
    "\n",
    "$Q_{t}$ = $KBDI_{t-1}$ − $Pnet_{t}$\n",
    "\n",
    "where: \n",
    "\n",
    "$KBDI_{t-1}$ is intialized to 0 everywhere at whatever time meets the wet condition criteria\n",
    "\n",
    "\n",
    "$$\n",
    "kbdi_{t} = (kbdi_{t-1}-pnet_{t}) + \\frac{(203.2 - kbdi_{t-1} - pnet_{t}) (0.968 e^{0.0875*tmax_{t} + 1.5552} - 8.30)* 10^{-3}}{1 + 10.88 e^{-0.001736*r}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a48bd1-d38e-4347-8ecc-c962286371a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need integers to use the array day_index for indexing\n",
    "# so we temporarily need to replace nans over ocean points with 0\n",
    "# we'll put them back later\n",
    "day_index_nonan = xr.where(~np.isfinite(day_index),0,day_index).astype('int').data\n",
    "np.unique(day_index_nonan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73920a24-70a8-4130-b032-6a7c41b66170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize KBDI array to all nan\n",
    "KBDI_np= pr.copy().compute().data\n",
    "KBDI_np[:,:,:]=np.nan\n",
    "\n",
    "# identify the KBDI t-1 initialization index by filling 0 at the appropriate index (day_index) for each cell\n",
    "# this is some real fancy indexing that will fill 0 at one single time in the timeseries for each grid cell\n",
    "# indexing this fancy only works on numpy arrays, not xarray structures, not dask arrays\n",
    "KBDI_np[day_index_nonan,np.arange(KBDI_np.shape[1])[:,None],np.arange(KBDI_np.shape[2])] = 0\n",
    "\n",
    "# put the ocean nans back using the landmask\n",
    "KBDI_np=np.where(landmask,KBDI_np,np.nan)\n",
    "\n",
    "# expecting shape (3653, 48, 48), unique vals (0,np.nan), and 1963 data points = 0\n",
    "KBDI_np.shape,np.unique(KBDI_np,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc39470-a258-4d33-b974-571c0889f44c",
   "metadata": {},
   "source": [
    "everything is as expected so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a20a40-003b-400c-a9e5-8070a4b6c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the equations for a single time first\n",
    "# at time t = 1\n",
    "Q = KBDI_np[0,:,:] - Pnet[1,:,:]\n",
    "print(Q.shape,np.unique(Q)) # should be all nan\n",
    "\n",
    "term2_numerator = (203.2 - Q) * (0.969 * np.exp(0.0875*tmax[1,:,:]+1.5552) - 8.3)\n",
    "term2_denominator = 1 + 10.88 * np.exp(-0.001736*mean_ann_pr)\n",
    "\n",
    "KBDI_np[1,:,:]= Q + (term2_numerator/term2_denominator)*10E-3\n",
    "\n",
    "# results should still be all nan or 0\n",
    "np.unique(KBDI_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d56afa-a0cb-4e3e-bc85-70d8382709b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# calculate KBDI for the whole timeseries at each grid\n",
    "\n",
    "term2_denominator = 1 + 10.88 * np.exp(-0.001736*mean_ann_pr)\n",
    "\n",
    "# looping through time\n",
    "for it in range(1,KBDI_np.shape[0]):\n",
    "\n",
    "    # KBDI calculation at every grid\n",
    "    Q = KBDI_np[it-1,:,:] - Pnet[it,:,:]\n",
    "    term2_numerator = (203.2 - Q) * (0.969 * np.exp(0.0875*tmax[it,:,:]+1.5552) - 8.3)\n",
    "    result = Q + (term2_numerator/term2_denominator)*10E-3\n",
    "    \n",
    "    # a mask of which grids are at or past the KBDI initialization index\n",
    "    mask = np.where(np.isfinite(KBDI_np[it-1,:,:]),1,0)\n",
    "\n",
    "    # fill result in only at grid cells where the initialization index has been reached\n",
    "    # this is necessarry in order to avoid propagating nan into the index we have initialized to 0 at each grid\n",
    "    KBDI_np[it,:,:] = np.where(mask,result,KBDI_np[it,:,:])\n",
    "    # print(it,KBDI_np[it,0,0])\n",
    "    \n",
    "    del Q, term2_numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50350ba2-7c62-4195-8618-439e421eb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to xarray\n",
    "KBDI=xr.DataArray(KBDI_np,coords=pr.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0c8cd-52f9-4b77-8cc0-45a8dafc232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KBDI.min().item(), KBDI.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d9ebc-b5bf-4605-a57c-e1732e67f950",
   "metadata": {},
   "source": [
    "Something is still going wrong because KBDI should be 0-800. The negatives are ok in the spin up right after initialization, but not getting any values above 203 is suspicious, even for this small test area.\n",
    "Testing on a larger area next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ba709-39a9-49de-a7f4-dd8b64d81eee",
   "metadata": {},
   "source": [
    "# Test on full dataset\n",
    "\n",
    "this works on a 10 year small spatial subset of the data, but is too RAM heavy for the full timeseries even at the small spatial subset. The problem is there are multiple things to process in parallel that don't get reduced (time dependency). I need to re-code this into a function that processes a single chunk, that is less RAM heavy, and that can be called with dask delayed, returning only the KBDI or writing KBDI directly to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca81ad0-1334-4cf9-a1da-762f2c52a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import LocalCluster,Client\n",
    "\n",
    "pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# pr_file = r'E://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# tmax_file = r'E://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "\n",
    "nworkers=16\n",
    "cluster = LocalCluster(n_workers=nworkers)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddb994-63cd-4bd8-b49f-14dd57d66232",
   "metadata": {},
   "source": [
    "The following parallel computations work but the KBDI values are wrong. Needs debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa38f4-ca6c-4739-b305-145b99fe7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_landmask(pr):\n",
    "    landmask=xr.where(np.isfinite(pr.mean('time')),1,0).astype('int8')\n",
    "    return landmask\n",
    "\n",
    "def calc_rr(pr,landmask):\n",
    "    rainmask=xr.where(pr>0,1,0).astype('int8')\n",
    "    rr=(rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "    rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "    return rr\n",
    "\n",
    "def create_categories(pr):\n",
    "    landmask = create_landmask(pr) \n",
    "    rr = calc_rr(pr,landmask) \n",
    "\n",
    "    # numpy arrays so that we can use fancy indexing\n",
    "    cat=da.where(rr>=3,5,rr) \n",
    "    day2_iii=np.argwhere(cat==2)\n",
    "    if np.isfinite(day2_iii.shape[0]):\n",
    "        for i in range(day2_iii.shape[0]):\n",
    "            cat[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "            cat[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "    cat=da.where(cat==1,0,cat)\n",
    "    cat=da.where(cat==5,1,cat)\n",
    "    # assert len(np.unique(cat))<=3,f'too many category values {np.unique(cat)}' # do a separate check function   \n",
    "    return xr.DataArray(cat,coords=pr.coords)\n",
    "\n",
    "def pnet_time_loop(cat_chunk,pr_chunk):\n",
    "    thresh = .20#5.08 # mm\n",
    "    thresh_flag=False\n",
    "    accpr=0.\n",
    "    pnet_chunk = np.where(cat_chunk==0, pr_chunk-thresh, np.float32(np.nan))\n",
    "    pnet_chunk = np.where(pnet_chunk<0,0,pnet_chunk)\n",
    "    for ilat in range(cat_chunk.shape[1]):\n",
    "        for ilon in range(cat_chunk.shape[2]):\n",
    "            for itime,(cat,pr) in enumerate(zip(cat_chunk[:,ilat,ilon],pr_chunk[:,ilat,ilon])):\n",
    "                if cat==0:\n",
    "                    accpr=0.\n",
    "                    thresh_flag=False\n",
    "                if cat==1:\n",
    "                    accpr=accpr+pr \n",
    "                    if (accpr<thresh) and (not thresh_flag):\n",
    "                        pnet_chunk[itime,ilat,ilon]=0\n",
    "                    elif (accpr>=thresh) and (not thresh_flag):\n",
    "                        accpr=accpr-thresh \n",
    "                        pnet_chunk[itime,ilat,ilon]=accpr\n",
    "                        thresh_flag=True\n",
    "                    else:\n",
    "                        pnet_chunk[itime,ilat,ilon]=accpr  \n",
    "    return pnet_chunk    \n",
    "\n",
    "def calc_pnet(pr,chunk):\n",
    "    print('getting rainfall categories..')\n",
    "    cat = create_categories(pr)\n",
    "    \n",
    "    cat_delayed = cat.data.to_delayed().ravel()\n",
    "    pr_delayed = pr.data.to_delayed().ravel()\n",
    "\n",
    "    tasklist = [dask.delayed(pnet_time_loop)(cat_chunk,pr_chunk) for cat_chunk,pr_chunk in zip(cat_delayed,pr_delayed)]\n",
    "    pnet_chunks = dask.compute(*tasklist)\n",
    "    pnet = xr.DataArray(np.concatenate(pnet_chunks,axis=2),coords=pr.coords) \n",
    "    return pnet#_chunks\n",
    "\n",
    "def get_init_index(pr):\n",
    "    ndays=7\n",
    "    pr_thresh=6 #152\n",
    "    pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "    day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "    return day_index\n",
    "\n",
    "def calc_mean_annual_pr(pr):\n",
    "    return pr.resample(time='YE').sum(min_count=10).mean('time')\n",
    "    \n",
    "def calc_KBDI(pr,tmax,pnet,init_ind,annpr,landmask):\n",
    "    KBDI= tmax.copy()\n",
    "    KBDI[:,:,:]=np.nan\n",
    "    init_ind = np.where(~np.isfinite(init_ind),0,init_ind).astype('int')\n",
    "    KBDI[init_ind,np.arange(KBDI.shape[1])[:,None],np.arange(KBDI.shape[2])] = 0\n",
    "    KBDI=np.where(landmask,KBDI,np.nan)\n",
    "    \n",
    "    # term2_denom = 1 + 10.88 * np.exp(-0.001736*annpr)\n",
    "    term2_denom = 1 + 10.88 * np.exp(-0.0441*annpr)\n",
    "\n",
    "    for it in range(1,KBDI.shape[0]):\n",
    "    \n",
    "        # KBDI calculation at every grid\n",
    "        # Q = KBDI[it-1,:,:] - pnet[it,:,:]\n",
    "        Q = KBDI[it-1,:,:] - pnet[it,:,:]*100.\n",
    "        # term2_num = (203.2 - Q) * (0.968 * np.exp(0.0875*tmax[it,:,:]+1.5552) - 8.3)\n",
    "        term2_num = (800 - Q) * (0.968 * np.exp(0.0486*tmax[it,:,:]) - 8.3)\n",
    "        result = Q + (term2_num/term2_denom)*10E-3\n",
    "        \n",
    "        # a mask of which grids are at or past the KBDI initialization index\n",
    "        mask = np.where(np.isfinite(KBDI[it-1,:,:]),1,0)\n",
    "    \n",
    "        # fill result in only at grid cells where the initialization index has been reached\n",
    "        # this is necessarry in order to avoid propagating nan into the index we have initialized to 0 at each grid\n",
    "        KBDI[it,:,:] = np.where(mask,result,KBDI[it,:,:])        \n",
    "    return KBDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6aad6-e2ef-426e-93ac-4845f6f49b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk3D={'time':-1,'lat':-1,'lon':25}\n",
    "chunk2D={'lat':-1,'lon':25}\n",
    "\n",
    "# persist pr data chunks across workers\n",
    "# pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960'),lat=slice(30,32),lon=slice(-90,-88)).round(2)\n",
    "pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960')).round(2)\n",
    "pr = pr/25.4 # mm --> inches\n",
    "time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "pr.coords['time_index']=('time',time_index)\n",
    "# pr=pr.persist()\n",
    "\n",
    "# persist tmax across workers\n",
    "# tmax=xr.open_dataset(tmax_file,chunks=chunk3D).tmax.sel(time=slice('1951','1960'),lat=slice(30,32),lon=slice(-90,-88)).round(2)\n",
    "tmax=xr.open_dataset(tmax_file,chunks=chunk3D).tmax.sel(time=slice('1951','1960')).round(2)\n",
    "tmax = tmax*(9./5.)+32.\n",
    "# tmax=tmax.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741d7fa-0290-420f-b61c-05450824c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# computing with dask delayed, returning xarray\n",
    "Pnet = calc_pnet(pr,chunk3D) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c23a9-73b7-406b-8769-fb07fea956e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "day_index = get_init_index(pr) # lazy,returns xarray\n",
    "mean_ann_pr = calc_mean_annual_pr(pr) #lazy,returns xarray\n",
    "\n",
    "# delay data chunks\n",
    "pr_delayed = pr.data.to_delayed().ravel()\n",
    "tmax_delayed = tmax.data.to_delayed().ravel()\n",
    "pnet_delayed = Pnet.chunk(chunk3D).data.to_delayed().ravel()\n",
    "init_ind_delayed = day_index.chunk(chunk2D).data.to_delayed().ravel()\n",
    "annpr_delayed = mean_ann_pr.chunk(chunk2D).data.to_delayed().ravel()\n",
    "landmask_delayed = create_landmask(pr).data.to_delayed().ravel()\n",
    "\n",
    "# verify everything has nchunks futures\n",
    "len(pr_delayed),len(tmax_delayed),len(pnet_delayed),len(init_ind_delayed),len(annpr_delayed),len(landmask_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0349f40-ffbb-41a7-9e94-ac380453bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# generate list of compute tasks\n",
    "tasklist = [dask.delayed(calc_KBDI)(pr_chunk,tmax_chunk,pnet_chunk,init_ind_chunk,annpr_chunk,mask_chunk) \n",
    "        for pr_chunk,tmax_chunk,pnet_chunk,init_ind_chunk,annpr_chunk,mask_chunk \n",
    "        in zip(pr_delayed,tmax_delayed,pnet_delayed,init_ind_delayed,annpr_delayed,landmask_delayed)] \n",
    "\n",
    "# compute\n",
    "KBDI_chunks = dask.compute(*tasklist)\n",
    "\n",
    "# numpy --> xarray\n",
    "KBDI = xr.DataArray(np.concatenate(KBDI_chunks,axis=2),coords=pr.coords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22304e2d-336b-4e2e-8230-971766ff28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_minmax(arr):\n",
    "    return arr.min().item(),arr.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f1827-2791-4caa-803d-aa4a5b29b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_minmax(KBDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7232c84-a63e-4eb3-a797-8626c67f4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_minmax(Pnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6a639-9d53-4a15-b377-c7d66e178a35",
   "metadata": {},
   "source": [
    "# Parallelize for less RAM usage\n",
    "\n",
    "function includes all I/O for a single chunk/file and no data is passed back\n",
    "\n",
    "each worker reads a file, computes, then writes out a new file to disk\n",
    "\n",
    "we can accomplish this either with dask.delayed (I think this is less confusing) or client.map\n",
    "\n",
    "\n",
    "The first few steps including calculating rainfall category, landmask, initialization index, and mean annual pr can be done with dask arrays. Try writing temp files after these steps. \n",
    "\n",
    "Temp file would contain:\n",
    "- cat (time,lat,lon),\n",
    "- landmask (lat,lon),\n",
    "- init_index (lat,lon),\n",
    "- pr_ann (lat,lon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d213b1-a709-444c-b071-15a041514921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import LocalCluster,Client\n",
    "import glob\n",
    "# pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# pr_file = r'E://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# tmax_file = r'E://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "# chunk3D={'time':-1,'lat':-1,'lon':25}\n",
    "\n",
    "nworkers=10\n",
    "cluster = LocalCluster(n_workers=nworkers)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517973f-bf6d-4be8-bbd7-2338e6363a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D://data/nclimgrid_daily/'\n",
    "pr_files=sorted(glob.glob(data_dir+'chunkedlon/prcp_nClimGridDaily_USsouth_*.nc'))\n",
    "tmax_files=sorted(glob.glob(data_dir+'chunkedlon/tmax_nClimGridDaily_USsouth_*.nc'))\n",
    "pr_files[0:3],len(pr_files),len(tmax_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b609b25-6578-425f-b7bc-2beea467cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in pr_files:\n",
    "    try:\n",
    "        xr.open_dataset(f,autoclose=True,lock=False)['prcp']\n",
    "    except:\n",
    "        print(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbbb1a-f801-467a-ac16-a16bc73d873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tmax_files:\n",
    "    try:\n",
    "        xr.open_dataset(f,autoclose=True,lock=False)['tmax']\n",
    "    except:\n",
    "        print(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb8be2-3c63-440f-a3d6-393f54fac6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_KBDI_file(pr_file,pr_varname,tmax_file,tmax_varname):\n",
    "    \n",
    "    # load data \n",
    "    chunk_id = pr_file[-6:-3]\n",
    "    print(chunk_id)\n",
    "    pr = xr.open_dataset(pr_file,autoclose=True,lock=False)[pr_varname] # units = mm\n",
    "    pr = pr/25.4 # mm --> inch\n",
    "    # with xr.open_dataset(pr_file)[pr_varname] as pr:\n",
    "        # pr = pr/25.4 # mm --> inch\n",
    "    time_index=np.arange(0,len(pr.time)).astype('int') # integer time\n",
    "    pr.coords['time_index']=('time',time_index)\n",
    "    tmax = xr.open_dataset(tmax_file,autoclose=True,lock=False)[tmax_varname] # units = C\n",
    "    tmax = tmax*(9./5.)+32. # C --> F\n",
    "    # with xr.open_dataset(tmax_file)[tmax_varname] as tmax:\n",
    "    #     tmax = tmax*(9./5.)+32. # C --> F\n",
    "\n",
    "    # create a landmask\n",
    "    landmask=xr.where(np.isfinite(pr.mean('time')),1,0).astype('int8')\n",
    "\n",
    "    # calc mean annual precip over a base period\n",
    "    pr_ann = pr.sel(time=slice('1981-01-01','2010-12-31')).resample(time='YE').sum(min_count=10).mean('time')\n",
    "\n",
    "    # # create rainfall categories, cat1 = consecutive rainfall day, cat0 = not consecutive rainfall day\n",
    "    # # start by finding rr, the number of consecutive days of rain\n",
    "    # rainmask=xr.where(pr>0,1,0).astype('int8')\n",
    "    # rr=(rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "    # del rainmask\n",
    "    # rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "    \n",
    "    # # now create categories (numpy arrays so that we can use fancy indexing)\n",
    "    # cat=np.where(rr>=3,5,rr) \n",
    "    # del rr\n",
    "    # day2_iii=np.argwhere(cat==2)\n",
    "    # if np.isfinite(day2_iii.shape[0]):\n",
    "    #     for i in range(day2_iii.shape[0]):\n",
    "    #         cat[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "    #         cat[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "    # del day2_iii\n",
    "    # cat=np.where(cat==1,0,cat)\n",
    "    # cat=np.where(cat==5,1,cat)\n",
    "    \n",
    "    # # calculation of Pnet for daily adjustment of the KBDI value\n",
    "    # thresh = .20 # inches\n",
    "    # thresh_flag=False\n",
    "    # accpr=0.\n",
    "    # # pnet for non-consecutive rain days\n",
    "    # pnet = np.where(cat==0, pr-thresh, np.float32(np.nan))\n",
    "    # pnet = np.where(pnet<0,0,pnet)\n",
    "    # # pnet for consecutive rain days\n",
    "    # for ilat in range(cat.shape[1]):\n",
    "    #     for ilon in range(cat.shape[2]):\n",
    "    #         for itime,(catval,prval) in enumerate(zip(cat[:,ilat,ilon],pr[:,ilat,ilon])):\n",
    "    #             if catval==0:\n",
    "    #                 accpr=0.\n",
    "    #                 thresh_flag=False\n",
    "    #             if catval==1:\n",
    "    #                 accpr=accpr+prval \n",
    "    #                 if (accpr<thresh) and (not thresh_flag):\n",
    "    #                     pnet[itime,ilat,ilon]=0\n",
    "    #                 elif (accpr>=thresh) and (not thresh_flag):\n",
    "    #                     accpr=accpr-thresh \n",
    "    #                     pnet[itime,ilat,ilon]=accpr\n",
    "    #                     thresh_flag=True\n",
    "    #                 else:\n",
    "    #                     pnet[itime,ilat,ilon]=accpr      \n",
    "\n",
    "    # # find the KBDI initialization index (when wet enough conditions are present)\n",
    "    # ndays=7 # in a one-week period\n",
    "    # pr_thresh=6 # need this many inches (152 mm also sometimes 8 inches 203 mm is used)\n",
    "    # pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "    # init_ind = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "\n",
    "    # # finally, the KBDI calculation\n",
    "    # # initialize\n",
    "    # KBDI= tmax.copy()\n",
    "    # KBDI[:,:,:]=np.nan\n",
    "    # # fill zero at the initialization index using very fancing indexing\n",
    "    # init_ind = np.where(~np.isfinite(init_ind),0,init_ind).astype('int')\n",
    "    # KBDI[init_ind,np.arange(KBDI.shape[1])[:,None],np.arange(KBDI.shape[2])] = 0\n",
    "    # KBDI=np.where(landmask,KBDI,np.nan)\n",
    "    \n",
    "    # # denominator of second term in KBDI equation\n",
    "    # term2_denom = 1 + 10.88 * np.exp(-0.0441*pr_ann)\n",
    "\n",
    "    # # loop through time\n",
    "    # for it in range(1,KBDI.shape[0]):\n",
    "    #     # KBDI value at time 'it-1'\n",
    "    #     Q = KBDI[it-1,:,:] - pnet[it,:,:]*100.\n",
    "    #     # numerator of the second term in KBDI equation\n",
    "    #     term2_num = (800 - Q) * (0.968 * np.exp(0.0486*tmax[it,:,:]) - 8.3)\n",
    "    #     # KBDI equation for time 'it'\n",
    "    #     result = Q + (term2_num/term2_denom)*10E-3\n",
    "         \n",
    "    #     # fill result in only at grid cells where the initialization index has been reached\n",
    "    #     # this is necessarry in order to avoid propagating nan into the index we have initialized to 0 at each grid\n",
    "    #     # use a mask of which grids are at or past the KBDI initialization index\n",
    "    #     mask = np.where(np.isfinite(KBDI[it-1,:,:]),1,0)        \n",
    "    #     KBDI[it,:,:] = np.where(mask,result,KBDI[it,:,:])   \n",
    "\n",
    "    # now write KBDI to file\n",
    "    # KBDI.to_netcdf(f'test_{chunk_id}.nc')\n",
    "    pr.to_netcdf(data_dir+f'kbdi/test_{chunk_id}.nc')\n",
    "    return chunk_id\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc092df-d548-495a-850a-b9a97cf401e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasklist = [dask.delayed(write_KBDI_file)(pf,'prcp',tf,'tmax') for pf,tf in zip(pr_files,tmax_files)]\n",
    "completed = dask.compute(*tasklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e924c-332c-4e9e-a54a-a563ccf0c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this also works\n",
    "nfiles = len(pr_files)\n",
    "completed = client.map(write_KBDI_file,*[pr_files,['prcp']*nfiles,tmax_files,['tmax']*nfiles])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9601d2-8366-42e7-bdf9-256ac52d0a8e",
   "metadata": {},
   "source": [
    "### also try dask.delayed with multiple functions\n",
    "\n",
    "this does not work because it tries to load all pr before writing any files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76247c-5709-4555-b0cc-0bdf1c519bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunkid(pr_file):\n",
    "    return pr_file[-5:-3]\n",
    "\n",
    "def load_pr(pr_file,pr_varname):\n",
    "    # load data \n",
    "    pr = xr.open_dataset(pr_file,autoclose=True)[pr_varname] # units = mm\n",
    "    pr = pr/25.4 # mm --> inch\n",
    "    time_index=np.arange(0,len(pr.time)).astype('int') # integer time\n",
    "    pr.coords['time_index']=('time',time_index)\n",
    "    return pr\n",
    "\n",
    "def load_tmax(tmax_file,tmax_varname):    \n",
    "    tmax = xr.open_dataset(tmax_file,autoclose=True)[tmax_varname] # units = C\n",
    "    tmax = tmax*(9./5.)+32. # C --> F\n",
    "    return tmax\n",
    "\n",
    "def create_landmask(pr):\n",
    "    # create a landmask\n",
    "    landmask=xr.where(np.isfinite(pr.mean('time')),1,0).astype('int8')\n",
    "    return landmask\n",
    "\n",
    "def calc_mean_annual_pr(pr):\n",
    "    # calc mean annual precip over a base period\n",
    "    pr_ann = pr.sel(time=slice('1981-01-01','2010-12-31')).resample(time='YE').sum(min_count=10).mean('time')\n",
    "    return pr_ann\n",
    "\n",
    "def calc_rr(pr):\n",
    "    # create rainfall categories, cat1 = consecutive rainfall day, cat0 = not consecutive rainfall day\n",
    "    # start by finding rr, the number of consecutive days of rain\n",
    "    rainmask=xr.where(pr>0,1,0).astype('int8')\n",
    "    rr=(rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "    del rainmask\n",
    "    rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "    return rr\n",
    "\n",
    "def write_file(arr,chunk_id):\n",
    "    arr.to_netcdf(f'test_{chunk_id}.nc')\n",
    "    return chunk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e661d9-94a0-4d19-a73d-47846a8e69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_id = [dask.delayed(get_chunkid)(f) for f in pr_files]\n",
    "pr = [dask.delayed(load_pr)(f,'prcp') for f in pr_files]\n",
    "landmask = [dask.delayed(create_landmask)(p) for p in pr]\n",
    "pr_ann = [dask.delayed(calc_mean_annual_pr)(p) for p in pr]\n",
    "rr = [dask.delayed(calc_rr)(p) for p in pr]\n",
    "completed = [dask.delayed(write_file)(r,id) for r,id in zip(rr,chunk_id)]\n",
    "completed=dask.compute(*completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850831f0-666a-4e21-aaf7-db15edfd3192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00313b-9c74-4b4f-9ea0-26bd65317da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed64b4a-0102-4149-a9db-10d50fdb8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21011be7-2bde-4557-9741-c2f2ce91424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9f0dc-c7e7-4b8b-a00c-77c4a4c48b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this will proceed as computations finish\n",
    "result=[]\n",
    "for future,arr in results_tuple:\n",
    "    result.append(arr)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab313e08-de5e-48b3-8b49-a2083e31b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2beb6-be62-4d6d-a77e-3bcef27afb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5cf35-a1d0-456b-8286-10270e4c1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# results = client.gather(futures)\n",
    "# results[0]\n",
    "futures[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3047abca-0fee-4920-993f-a7707bbe3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures[62].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17929ee-6e1b-4244-b06b-60a587a774d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# dask delayed errors because it holds onto results, we need client.map so we can return results as they complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7c6ef-78ac-4a9e-9614-19e0f57b6662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6163e-be3c-4258-9e4d-33a4f4dd6430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eee83d-e2dd-4a38-b7c1-1a0da1cfea86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae9125-efa9-4371-a8f5-65585c7073fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb3b3c-7494-4f33-8583-fad8b2d5c53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2b148-02cd-4c1a-ad85-57c6a9d252b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93297a5-0581-4235-8095-dac9b2732820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cccd49-8e6d-4890-8287-7792102683f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db8396-4c1c-4c6a-ac75-626ddf565772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737c617-a813-4614-8be1-9dde15d84969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50614cb-d51c-4c18-9bac-ebb048f5c52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32164e-9e46-4cca-83db-9228e2cda18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2afe6-44e2-4674-a81e-81c172d82f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc155fad-aad8-407d-b57d-4a6108e4e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e83b8-38f2-4b28-b5ca-0e5062b5c020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f588da-5281-45b5-9063-59de7266c440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96bede-5567-4acc-b3eb-bb7ed9976654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this errors in the dashboard\n",
    "# code taken from https://stackoverflow.com/questions/45941528/how-to-efficiently-send-a-large-numpy-array-to-the-cluster-with-dask-array\n",
    "def load_precip(filename,chunk):\n",
    "    return xr.open_mfdataset(filename,chunks=chunk)\n",
    "\n",
    "ds = dask.delayed(load_precip)(pr_file,chunk3D)\n",
    "ds = da.from_delayed(ds,shape=(26907,358,753),dtype='float32')\n",
    "ds = ds.rechunk((26907,358,25))\n",
    "ds = ds.persist()\n",
    "ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0292d05-3c5b-4a66-80db-baeea09dfb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this sends the whole ds to a single worker\n",
    "ds = xr.open_mfdataset(pr_file,chunks=chunk3D)\n",
    "ds_futures = client.scatter(ds)\n",
    "client.who_has(ds_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915953e2-9a8a-4f31-9763-7b95347d17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(pr_file,chunks=chunk3D)\n",
    "ds = da.from_array(ds.data,shape=(26907,358,753),dtype='float32')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cd6c9-79e6-464c-aeef-917415245900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c63abaa-98e2-4203-aacf-9caea3725b3b",
   "metadata": {},
   "source": [
    "# fully dask delayed from separate data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c10f81-a6d8-4d49-9b24-585651d306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precip(filename,varname):\n",
    "    return xr.open_dataset(filename)[varname]\n",
    "\n",
    "def create_landmask(pr):\n",
    "    landmask=xr.where(np.isfinite(pr.mean('time')),1,0).astype('int8')\n",
    "    return landmask\n",
    "\n",
    "def calc_rr(pr,landmask):\n",
    "    rainmask=xr.where(pr>0,1,0).astype('int8')\n",
    "    rr=(rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "    rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "    return rr\n",
    "\n",
    "def create_categories(pr,landmask):\n",
    "    # landmask = create_landmask(pr) \n",
    "    rr = calc_rr(pr,landmask) \n",
    "\n",
    "    # numpy arrays so that we can use fancy indexing\n",
    "    cat=da.where(rr>=3,5,rr) \n",
    "    day2_iii=np.argwhere(cat==2)\n",
    "    if np.isfinite(day2_iii.shape[0]):\n",
    "        for i in range(day2_iii.shape[0]):\n",
    "            cat[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "            cat[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "    cat=da.where(cat==1,0,cat)\n",
    "    cat=da.where(cat==5,1,cat)\n",
    "    # assert len(np.unique(cat))<=3,f'too many category values {np.unique(cat)}' # do a separate check function   \n",
    "    return xr.DataArray(cat,coords=pr.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d6ced-8227-4d2e-8af6-a466a6e06d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466407c-37af-4c56-9547-c91ef131a51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2e8b6-b207-4f5d-8705-dea13fb0e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = [dask.delayed(load_precip)(f,'prcp') for f in pr_files]\n",
    "pr = dask.compute(pr)\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5233a-d47b-432f-9b05-f8432d0754a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164b031-a34e-4093-94c9-b53c6e9de757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_landmask(pr):\n",
    "    landmask=xr.where(np.isfinite(pr.mean('time')),1,0).astype('int8')\n",
    "    return landmask\n",
    "\n",
    "def calc_rr(pr,landmask):\n",
    "    rainmask=xr.where(pr>0,1,0).astype('int8')\n",
    "    rr=(rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "    rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "    return rr\n",
    "\n",
    "def create_categories(pr,landmask):\n",
    "    # landmask = create_landmask(pr) \n",
    "    rr = calc_rr(pr,landmask) \n",
    "\n",
    "    # numpy arrays so that we can use fancy indexing\n",
    "    cat=da.where(rr>=3,5,rr) \n",
    "    day2_iii=np.argwhere(cat==2)\n",
    "    if np.isfinite(day2_iii.shape[0]):\n",
    "        for i in range(day2_iii.shape[0]):\n",
    "            cat[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "            cat[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "    cat=da.where(cat==1,0,cat)\n",
    "    cat=da.where(cat==5,1,cat)\n",
    "    # assert len(np.unique(cat))<=3,f'too many category values {np.unique(cat)}' # do a separate check function   \n",
    "    return xr.DataArray(cat,coords=pr.coords)\n",
    "\n",
    "def get_init_index(pr):\n",
    "    ndays=7\n",
    "    pr_thresh=6 #152\n",
    "    time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "    pr.coords['time_index']=('time',time_index)\n",
    "    pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "    day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "    return day_index\n",
    "\n",
    "def calc_mean_annual_pr(pr):\n",
    "    return pr.resample(time='YE').sum(min_count=10).mean('time')\n",
    "\n",
    "\n",
    "# def pr_mm_to_inch(pr):\n",
    "#     return pr.copy()/25.4\n",
    "\n",
    "def load_precip(filename,varname,chunk):\n",
    "    return xr.open_mfdataset(filename,chunks=chunk)[varname].round(2)\n",
    "    \n",
    "\n",
    "def write_temporary_file(pr_filename,pr_varname,chunk3D,base_start,base_end):\n",
    "    print('loading pr...')\n",
    "    pr = load_precip(pr_filename,pr_varname,chunk3D)\n",
    "    print('adjusting pr units...')\n",
    "    pr = pr/25.4 # mm to inch\n",
    "    # print('generating landmask...')\n",
    "    # landmask = create_landmask(pr.sel(time=slice(base_start,base_end)))\n",
    "    # print('creating pr categories...')\n",
    "    # cat = create_categories(pr,landmask)\n",
    "    # print('finding the initialization index...')\n",
    "    # init_index = get_init_index(pr)\n",
    "    # print('calculating mean annual pr...')\n",
    "    # pr_ann = calc_mean_annual_pr(pr.sel(time=slice(base_start,base_end)))\n",
    "\n",
    "    # dsout = init_index.to_dataset(name='init_index')\n",
    "    print('writing file...')\n",
    "    pr.to_netcdf('test.nc')\n",
    "    \n",
    "    # return pr #pr_ann\n",
    "# pr = pr/25.4 # mm --> inches\n",
    "# time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "# pr.coords['time_index']=('time',time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0152c6c-ff57-442a-8892-4e1babd02fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk3D={'time':-1,'lat':-1,'lon':25}\n",
    "# chunk2D=dict((key,chunk3D[key]) for key in ['lat','lon'])\n",
    "# chunk2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67217ec-f9d5-4316-b75c-1d931a7f9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work. Takes forever, output is 28G file, is that expected? May need to try client.map or dask delayed\n",
    "write_temporary_file(pr_file,'prcp',{'time':-1,'lat':-1,'lon':10},'1980-01-01','2010-01-01').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2389299-7358-41c1-b7bb-d5df6bb217db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae7bc2-d833-4697-a6e1-b4f0fff9f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp#.sel(time=slice('1951','1960')).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df12fa6-3fd3-44f8-b048-ad8a0397a2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa28119-4b93-4b2a-a0a2-80079beb88d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082846f-9238-46e0-859b-7fc99d13c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('lazy load pr...')\n",
    "chunk3D={'time':-1,'lat':-1,'lon':25}\n",
    "pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960')).round(2)\n",
    "time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "pr.coords['time_index']=('time',time_index)\n",
    "# pr=pr.compute()\n",
    "pr=pr.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bd33e-a5df-4716-84d3-a395ce57735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('lazy landmask...')\n",
    "landmask=xr.where(np.isfinite(pr.mean('time')),1,0).astype('int8')#.compute()\n",
    "nlandpts = landmask.sum().data\n",
    "print(nlandpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cb259-2577-4027-9f39-d206eafc04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('lazy finding initialization index...')\n",
    "ndays=7\n",
    "pr_thresh=152\n",
    "pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()#.compute()\n",
    "day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')#.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4883ace-3113-46b6-bd77-925e5f60097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0766d1-cae3-4e1b-ab4a-feafa4e76b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lazy identifying consecutive day rainfall...')\n",
    "consecday=xr.where(pr>0,1,0).astype('int8').cumsum('time')\n",
    "rr=(consecday-consecday.where(consecday == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "# rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "rr=rr.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d23328-e448-4ac7-bd57-fafd346204e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b738c12-c3d3-4a40-b474-7bb7ac44225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lazy landmask...')\n",
    "chunk3D={'time':-1,'lat':-1,'lon':25}\n",
    "prsub=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960'))\n",
    "landmask=xr.where(np.isfinite(prsub.mean('time')),1,0).astype('int8')\n",
    "nlandpts = landmask.sum().data\n",
    "# print(f'{nlandpts} data points (grids) over land')\n",
    "\n",
    "print('lazy finding initialization index...')\n",
    "ndays=7\n",
    "pr_thresh=152\n",
    "pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "\n",
    "print('lazy identifying consecutive day rainfall...')\n",
    "consecday=xr.where(pr>0,1,0).astype('int8').cumsum('time')\n",
    "rr=(consecday-consecday.where(consecday == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "# rr=rr.compute()\n",
    "rr=rr.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a8932-9eac-482b-ba0a-4a34d26300ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dc21e-6d0a-4060-89a8-045ef17a0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_chunk(rr_chunk,pr_chunk):\n",
    "    \n",
    "#     def create_categories(rr_chunk):\n",
    "#         cat_chunk=xr.where(rr_chunk>=3,5,rr_chunk)\n",
    "#         day2_iii=np.argwhere(rr_chunk==2)\n",
    "#         for i in range(day2_iii.shape[0]):\n",
    "#             cat_chunk[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "#             cat_chunk[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "#         cat_chunk=xr.where(cat_chunk==1,0,cat_chunk)\n",
    "#         cat_chunk=xr.where(cat_chunk==5,1,cat_chunk)\n",
    "#         assert len(np.unique(cat_chunk))<=3,f'too many category values {np.unique(cat_chunk)}' \n",
    "#         return cat_chunk\n",
    "\n",
    "#     def calc_pnet(cat_chunk,pr_chunk):\n",
    "#         thresh = 5.08 # mm\n",
    "#         thresh_flag=False\n",
    "#         accpr=0.\n",
    "#         pnet = np.where(cat_chunk==0, pr_chunk-thresh, np.nan)\n",
    "#         pnet = np.where(pnet<0,0,pnet)\n",
    "#         for ilat in range(cat_chunk.shape[1]):\n",
    "#             for ilon in range(cat_chunk.shape[2]):\n",
    "#                 for itime,(cat,pr) in enumerate(zip(cat_chunk[:,ilat,ilon],pr_chunk[:,ilat,ilon])):\n",
    "#                     if cat==0:\n",
    "#                         accpr=0.\n",
    "#                         thresh_flag=False\n",
    "#                     if cat==1:\n",
    "#                         accpr=accpr+pr \n",
    "#                         if (accpr<thresh) and (not thresh_flag):\n",
    "#                             pnet[itime,ilat,ilon]=0\n",
    "#                         elif (accpr>=thresh) and (not thresh_flag):\n",
    "#                             accpr=accpr-thresh \n",
    "#                             pnet[itime,ilat,ilon]=accpr\n",
    "#                             thresh_flag=True\n",
    "#                         else:\n",
    "#                             pnet[itime,ilat,ilon]=accpr  \n",
    "#         return pnet\n",
    "\n",
    "#     cat_chunk = create_categories(rr_chunk)\n",
    "#     del rr_chunk\n",
    "#     Pnet = calc_pnet(cat_chunk,pr_chunk)\n",
    "#     return Pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e060bf-8e8c-410f-9b49-cbce6cdbe010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48cc55d-428a-435e-97f1-efe996ff230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def create_categories(rr_chunk):\n",
    "    cat_chunk=xr.where(rr_chunk>=3,5,rr_chunk)\n",
    "    day2_iii=np.argwhere(rr_chunk==2)\n",
    "    for i in range(day2_iii.shape[0]):\n",
    "        cat_chunk[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "        cat_chunk[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "    cat_chunk=xr.where(cat_chunk==1,0,cat_chunk)\n",
    "    cat_chunk=xr.where(cat_chunk==5,1,cat_chunk)\n",
    "    assert len(np.unique(cat_chunk))<=3,f'too many category values {np.unique(cat_chunk)}' \n",
    "    return cat_chunk\n",
    "\n",
    "def calc_pnet(cat_chunk,pr_chunk):\n",
    "    thresh = 5.08 # mm\n",
    "    thresh_flag=False\n",
    "    accpr=0.\n",
    "    pnet = np.where(cat_chunk==0, pr_chunk-thresh, np.nan)\n",
    "    pnet = np.where(pnet<0,0,pnet)\n",
    "    for ilat in range(cat_chunk.shape[1]):\n",
    "        for ilon in range(cat_chunk.shape[2]):\n",
    "            for itime,(cat,pr) in enumerate(zip(cat_chunk[:,ilat,ilon],pr_chunk[:,ilat,ilon])):\n",
    "                if cat==0:\n",
    "                    accpr=0.\n",
    "                    thresh_flag=False\n",
    "                if cat==1:\n",
    "                    accpr=accpr+pr \n",
    "                    if (accpr<thresh) and (not thresh_flag):\n",
    "                        pnet[itime,ilat,ilon]=0\n",
    "                    elif (accpr>=thresh) and (not thresh_flag):\n",
    "                        accpr=accpr-thresh \n",
    "                        pnet[itime,ilat,ilon]=accpr\n",
    "                        thresh_flag=True\n",
    "                    else:\n",
    "                        pnet[itime,ilat,ilon]=accpr  \n",
    "    return pnet\n",
    "\n",
    "def calc_KBDI(pnet_chunk,tmax_chunk,pr_ann_avg,day_index_chunk,landmask_chunk):\n",
    "\n",
    "    KBDI= tmax_chunk.copy()\n",
    "    KBDI[:,:,:]=np.nan\n",
    "    KBDI[day_index_chunk,np.arange(KBDI.shape[1])[:,None],np.arange(KBDI.shape[2])] = 0\n",
    "    KBDI=np.where(landmask_chunk,KBDI,np.nan)\n",
    "    \n",
    "    term2_denom = 1 + 10.88 * np.exp(-0.001736*pr_ann_avg)\n",
    "\n",
    "    for it in range(1,KBDI.shape[0]):\n",
    "    \n",
    "        # KBDI calculation at every grid\n",
    "        Q = KBDI[it-1,:,:] - pnet_chunk[it,:,:]\n",
    "        term2_num = (203.2 - Q) * (0.969 * np.exp(0.0875*tmax_chunk[it,:,:]+1.5552) - 8.3)\n",
    "        result = Q + (term2_num/term2_denom)*10E-3\n",
    "        \n",
    "        # a mask of which grids are at or past the KBDI initialization index\n",
    "        mask = np.where(np.isfinite(KBDI[it-1,:,:]),1,0)\n",
    "    \n",
    "        # fill result in only at grid cells where the initialization index has been reached\n",
    "        # this is necessarry in order to avoid propagating nan into the index we have initialized to 0 at each grid\n",
    "        KBDI[it,:,:] = np.where(mask,result,KBDI[it,:,:])        \n",
    "        del Q, term2_num, result, mask\n",
    "    return KBDI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dbcc8-b159-453c-b4c0-a9a728585ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('lazy load pr...')\n",
    "chunk3D={'time':-1,'lat':-1,'lon':50}\n",
    "pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960')).round(2)\n",
    "time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "pr.coords['time_index']=('time',time_index)\n",
    "\n",
    "print('lazy landmask...')\n",
    "prsub=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960'))\n",
    "landmask=xr.where(np.isfinite(prsub.mean('time')),1,0).astype('int8')\n",
    "nlandpts = landmask.sum().data\n",
    "# print(f'{nlandpts} data points (grids) over land')\n",
    "\n",
    "print('lazy finding initialization index...')\n",
    "ndays=7\n",
    "pr_thresh=152\n",
    "pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "\n",
    "print('lazy identifying consecutive day rainfall...')\n",
    "rainmask=xr.where(pr>0,1,0).astype('int8')\n",
    "rr=(rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)).astype('float32')\n",
    "rr=xr.where(landmask,rr,np.float32(np.nan)).transpose('time','lat','lon') \n",
    "# rr=rr.compute()\n",
    "rr=rr.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bc099-8329-420f-83f5-267467baaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e6a48-b5fe-4e14-8e29-ff256564f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tmax=xr.open_dataset(tmax_file).tmax.sel(time=slice('1951','1960'),lat=slice(30,32),lon=slice(-90,-88)).round(2).load()\n",
    "# mean_ann_pr = pr.resample(time='YE').sum(min_count=10).mean('time').compute()\n",
    "\n",
    "\n",
    "cat_chunk = create_categories(rr_chunk)\n",
    "del rr_chunk\n",
    "Pnet_chunk = calc_pnet(cat_chunk,pr_chunk)\n",
    "KBDI_chunk = calc_KBDI(Pnet_chunk,tmax_chunk,mean_ann_pr,day_index_chunk,landmask_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31e53f-7c8a-45e3-803a-00dd318f1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_chunk(pr_chunk):\n",
    "#     def create_categories(rr_chunk):\n",
    "#         cat_chunk=xr.where(rr_chunk>=3,5,rr_chunk)\n",
    "#         day2_iii=np.argwhere(rr_chunk==2)\n",
    "#         for i in range(day2_iii.shape[0]):\n",
    "#             cat_chunk[day2_iii[i,0],day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "#             cat_chunk[day2_iii[i,0]-1,day2_iii[i,1],day2_iii[i,2]] = 5    \n",
    "#         cat_chunk=xr.where(cat_chunk==1,0,cat_chunk)\n",
    "#         cat_chunk=xr.where(cat_chunk==5,1,cat_chunk)\n",
    "#         assert len(np.unique(cat_chunk))<=3,f'too many category values {np.unique(cat_chunk)}' \n",
    "#         return cat_chunk\n",
    "\n",
    "#     cat_chunk = create_categories(rr_chunk)\n",
    "    \n",
    "#     thresh = 5.08 # mm\n",
    "#     thresh_flag=False\n",
    "#     accpr=0.\n",
    "#     pnet = np.where(cat_chunk==0, pr_chunk-thresh, np.nan)\n",
    "#     pnet = np.where(pnet<0,0,pnet)\n",
    "#     for ilat in range(cat_chunk.shape[1]):\n",
    "#         for ilon in range(cat_chunk.shape[2]):\n",
    "#             for itime,(cat,pr) in enumerate(zip(cat_chunk[:,ilat,ilon],pr_chunk[:,ilat,ilon])):\n",
    "#                 if cat==0:\n",
    "#                     accpr=0.\n",
    "#                     thresh_flag=False\n",
    "#                 if cat==1:\n",
    "#                     accpr=accpr+pr \n",
    "#                     if (accpr<thresh) and (not thresh_flag):\n",
    "#                         pnet[itime,ilat,ilon]=0\n",
    "#                     elif (accpr>=thresh) and (not thresh_flag):\n",
    "#                         accpr=accpr-thresh \n",
    "#                         pnet[itime,ilat,ilon]=accpr\n",
    "#                         thresh_flag=True\n",
    "#                     else:\n",
    "#                         pnet[itime,ilat,ilon]=accpr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65e576-9e0f-4f52-8ddd-2daa077e2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('lazy load pr...')\n",
    "chunk3D={'time':-1,'lat':-1,'lon':50}\n",
    "pr=xr.open_mfdataset(pr_file,chunks=chunk3D).prcp.sel(time=slice('1951','1960')).round(2)#.load()\n",
    "# pr=xr.open_dataset(pr_file).prcp.sel(time=slice('1951','1960')).round(2)#.load()\n",
    "time_index=np.arange(0,len(pr.time)).astype('int')\n",
    "pr.coords['time_index']=('time',time_index)\n",
    "\n",
    "print('compute landmask...')\n",
    "prsub=xr.open_dataset(pr_file).prcp.sel(time=slice('1951','1960')).round(2).load()\n",
    "landmask=xr.where(np.isfinite(prsub.mean('time')),1,0)#.compute()\n",
    "nlandpts = landmask.sum().data\n",
    "print(f'{nlandpts} data points (grids) over land')\n",
    "\n",
    "print('lazy finding initialization index...')\n",
    "ndays=7\n",
    "pr_thresh=152\n",
    "pr_weeksum=pr.rolling(time=ndays,min_periods=4,center=False).sum()\n",
    "day_index = xr.where(pr_weeksum>pr_thresh,pr_weeksum.time_index,np.nan).min('time')\n",
    "# assert day_index.min().data >= 7,f'found intialization at index {day_index.min().data.item()}'\n",
    "\n",
    "# nnan_mask=landmask.sum().data # nans in the landmask (ocean points)\n",
    "# nnan_data=xr.where(np.isfinite(day_index),1,0).sum().data # nan grids in our initialization index (should be only ocean points)\n",
    "# assert nnan_mask == nnan_data, f'there are {nnan_data - nnan_mask} grid cells where a KBDI initialization index was not found'\n",
    "\n",
    "print('identifying consecutive day rainfall...')\n",
    "rainmask=xr.where(pr>0,1,0)\n",
    "rr=rainmask.cumsum('time')-rainmask.cumsum('time').where(rainmask == 0).ffill(dim='time').fillna(0)\n",
    "rr=xr.where(landmask,rr,np.nan).transpose('time','lat','lon') \n",
    "\n",
    "rr_chunked=rr.chunk(chunk3D)\n",
    "rr_delayed = rr_chunked.data.to_delayed().ravel()\n",
    "pr_delayed = pr.data.to_delayed().ravel()\n",
    "\n",
    "task_list=[dask.delayed(process_chunk)(rr_chunk,pr_chunk) for rr_chunk,pr_chunk in zip(rr_delayed,pr_delayed)]\n",
    "print('computing in parallel...')\n",
    "result_chunks=dask.compute(*task_list)\n",
    "pnet_np=np.concatenate(result_chunks,axis=2)\n",
    "Pnet=xr.DataArray(pnet_np,coords=pr.coords)\n",
    "Pnet\n",
    "\n",
    "\n",
    "# # task_list=[dask.delayed(create_categories)(rr_chunk) for rr_chunk in rr_delayed]\n",
    "# # result_chunks=dask.compute(*task_list)\n",
    "# # cat_np=np.concatenate(result_chunks,axis=2)\n",
    "# # cat=xr.DataArray(cat_np,coords=rr.coords)\n",
    "# # cat\n",
    "\n",
    "# thresh = 5.08\n",
    "# # chunk, delay, ravel\n",
    "# # chunk={'time':-1,'lat':-1,'lon':3}\n",
    "# cat=cat.chunk(chunk3D)\n",
    "# # pr=pr.chunk(chunk)\n",
    "# cat_delayed=cat.data.to_delayed().ravel()\n",
    "# pr_delayed=pr.data.to_delayed().ravel()\n",
    "# len(cat_delayed),len(pr_delayed)\n",
    "# task_list=[dask.delayed(calc_pnet)(cat_chunk,pr_chunk) for cat_chunk,pr_chunk in zip(cat_delayed,pr_delayed)]\n",
    "# result_chunks=dask.compute(*task_list)\n",
    "# pnet_np=np.concatenate(result_chunks,axis=2)\n",
    "# Pnet=xr.DataArray(pnet_np,coords=pr.coords)\n",
    "\n",
    "\n",
    "# chunk2D = {'lat':-1,'lon':3}\n",
    "# Pnet_delayed = Pnet.chunk(chunk).data.to_delayed().ravel()\n",
    "# day_index_delayed = day_index.chunk(chunk2D).data.to_delayed().ravel()\n",
    "# landmask_delayed = landmask.chunk(chunk2D).data.to_delayed().ravel()\n",
    "# print(len(Pnet_delayed),len(day_index_delayed),len(landmask_delayed))\n",
    "# task_list=[dask.delayed(fill_nan_times)(pnet_chunk,dayind_chunk,mask_chunk) for pnet_chunk,dayind_chunk,mask_chunk in zip(Pnet_delayed,day_index_delayed,landmask_delayed)]\n",
    "# result_chunks=dask.compute(*task_list)\n",
    "# pnet_np=np.concatenate(result_chunks,axis=2)\n",
    "# Pnet=xr.DataArray(pnet_np,coords=pr.coords)\n",
    "\n",
    "# tmax=xr.open_dataset(tmax_file).tmax.sel(time=slice('1951','1960'),lat=slice(30,32),lon=slice(-90,-88)).round(2).load()\n",
    "# mean_ann_pr = pr.resample(time='YE').sum(min_count=10).mean('time').compute()\n",
    "\n",
    "\n",
    "# day_index_nonan = xr.where(~np.isfinite(day_index),0,day_index).astype('int').data\n",
    "# KBDI_np= pr.copy().compute().data\n",
    "# KBDI_np[:,:,:]=np.nan\n",
    "# KBDI_np[day_index_nonan,np.arange(KBDI_np.shape[1])[:,None],np.arange(KBDI_np.shape[2])] = 0\n",
    "# KBDI_np=np.where(landmask,KBDI_np,np.nan)\n",
    "# KBDI_np.shape,np.unique(KBDI_np,return_counts=True)\n",
    "\n",
    "# term2_denominator = 1 + 10.88 * np.exp(-0.001736*mean_ann_pr)\n",
    "\n",
    "# # looping through time\n",
    "# for it in range(1,KBDI_np.shape[0]):\n",
    "\n",
    "#     # KBDI calculation at every grid\n",
    "#     Q = KBDI_np[it-1,:,:] - Pnet[it,:,:]\n",
    "#     term2_numerator = (203.2 - Q) * (0.969 * np.exp(0.0875*tmax[it,:,:]+1.5552) - 8.3)\n",
    "#     result = Q + (term2_numerator/term2_denominator)*10E-3\n",
    "    \n",
    "#     # a mask of which grids are at or past the KBDI initialization index\n",
    "#     mask = np.where(np.isfinite(KBDI_np[it-1,:,:]),1,0)\n",
    "\n",
    "#     # fill result in only at grid cells where the initialization index has been reached\n",
    "#     # this is necessarry in order to avoid propagating nan into the index we have initialized to 0 at each grid\n",
    "#     KBDI_np[it,:,:] = np.where(mask,result,KBDI_np[it,:,:])\n",
    "#     # print(it,KBDI_np[it,0,0])\n",
    "    \n",
    "#     del Q, term2_numerator\n",
    "\n",
    "# KBDI=xr.DataArray(KBDI_np,coords=pr.coords)\n",
    "# KBDI.min().item(), KBDI.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516011d2-fffa-404f-9572-cfd531cc7f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcae45e-03bc-479a-842c-62a4e3008b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1355b-eb41-4c65-8db1-6a6e80b81369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ae70a-b3ff-4bfe-b062-51adcc268c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87643c06-5dee-4ba9-ae29-3be99233e0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19dcabb-273d-4714-bc24-eedfb2787514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978bd047-a5b3-4da8-abda-2ec51746e91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eec1b5-03d9-4085-9beb-0aaa7595bd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af554da-cace-43a5-a92b-6c7d79f53c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8468bd-466f-4f19-b8c5-7ac56177ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0a1d7-054d-4845-a3c8-98b390ca7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all nans, debugging on a single grid\n",
    "\n",
    "KBDI_np= pr.copy().compute().data\n",
    "KBDI_np[:,:,:]=np.nan\n",
    "KBDI_np[day_index_nonan,np.arange(KBDI_np.shape[1])[:,None],np.arange(KBDI_np.shape[2])] = 0\n",
    "KBDI_np=np.where(landmask,KBDI_np,np.nan)\n",
    "\n",
    "KBDIsub=KBDI_np[:,0,0].copy()\n",
    "Pnetsub=Pnet[:,0,0].data.copy()\n",
    "tmaxsub=tmax[:,0,0].data.copy()\n",
    "map_sub=mean_ann_pr[0,0].data.copy()\n",
    "\n",
    "print(KBDIsub.shape,Pnetsub.shape,tmaxsub.shape,map_sub.shape)\n",
    "\n",
    "term2_denominator = 1 + 10.88 * np.exp(-0.001736*map_sub)\n",
    "\n",
    "for it in range(1,KBDIsub.shape[0]):\n",
    "    if np.isfinite(KBDIsub[it-1]):\n",
    "        Q = KBDIsub[it-1] - Pnetsub[it]\n",
    "        term2_numerator = ((203.2 - Q) * (0.969 * np.exp(0.0875*tmaxsub[it]+1.5552) - 8.3))#.astype('float32')\n",
    "        KBDIsub[it] = (Q + (term2_numerator/term2_denominator)*10E-3).astype('float32')\n",
    "        print(it,Q,term2_numerator,KBDIsub[it])\n",
    "        del Q, term2_numerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4adbd7-504e-4afc-939d-2c3081dd0a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef70cc-83d8-4daf-a1db-879c35196e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cacd16-250b-4ee0-be62-429d172988da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b05869-895e-40a5-ad24-3971f3b18aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd88aba-025c-4c84-b9d8-5c0ffad32efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f30cd-d4b6-4451-a8f6-55b20629c319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408faf4-b114-4f37-931a-64b43990b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "KBDI[:,0,0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c95765-3a8a-4081-9622-a4a01b4d7593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79ca39-f8dc-418a-9738-03829a5c5174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e6461-4ca4-44db-ab96-1bbbb6849180",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indices = np.random.randint(0, 10, (5, 5))\n",
    "time_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7beac-76e8-48dc-b108-6921a20c37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(5)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12496126-67f7-46c8-b2d2-17fc5d8de988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(10, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f9fb5-727d-4c29-9a68-11a5184fae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[time_indices,np.arange(5)[:,None],np.arange(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02a428-3cab-4ffe-ac1a-4401c19fcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[7,0,0],data[9,0,1],data[1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c7bda-dcfb-4253-b85b-bada8dff7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet.data[day_index_nonan+1,np.arange(len(Pnet.lat))[:,None],np.arange(len(Pnet.lon))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8256c44-ae65-42c1-8825-dbae1008af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet.time[int(day_index.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe3123-5d61-4e33-acee-f5a63294b607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ca5d7-6b81-490e-bf4c-ef0bb536c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Pnet.copy()\n",
    "test[:day_index_nonan+1,np.arange(len(Pnet.lat))[:,None],np.arange(len(Pnet.lon))] = np.nan\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899d516-e9a1-4e2c-b6c1-5f3959af4e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7de7d3-baa9-4fc7-8664-e2c7e0104f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fcd36-3608-446a-ac7a-969be5226b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da32fcc-a78b-4b36-ba09-1542b133a4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddc462-592e-4f1b-bfff-30ec2624a924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1141cb-dcce-4eea-9579-80bf05caf56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8208e-f0f4-4420-b2bc-2445b1860534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53859a-95b2-4aa2-9e0f-9efffda494d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "KBDI of t = Q + ( (800 - Q) * (0.968 * e^(0.048 6 *T) - 8.30) - 8.30) * Δt ) / ( 1 + 10.88 * e^(-0.0441 * P) ) * 10^(-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e5b96-3b2a-47c3-aa06-2197d8b84e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet_sub=rr[:,0,0].copy()\n",
    "Pnet_sub[:]=np.nan\n",
    "rr_sub=rr[:,0,0]\n",
    "Pnet_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172d24a-1eba-4334-94dc-e88d89fa8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_sub[0:20].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab09da-ca8c-4ffe-951f-a3b9fdc837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(Pnet_sub.time[0:20])):\n",
    "    # has there been rain for consecutive days\n",
    "    if rr_sub[d] >= 2:\n",
    "        \n",
    "    for nd in range(int(rr_sub[d].data)-int(rr_sub[d].data),int(rr_sub[d].data)):\n",
    "        print(d,nd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea051c-c758-4d23-9524-34d6679d65ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa9f6c-834f-4faf-86f2-b4e6bcc4c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d0375-e30f-41be-840e-be9313aa0bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnbydoing",
   "language": "python",
   "name": "learnbydoing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
