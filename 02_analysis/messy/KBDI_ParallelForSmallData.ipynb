{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a869a0e6-b6d7-4bba-9b4a-93429bca4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "import dask\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32aa6680-ef24-42f9-8730-11440f822c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS VERSION WORKS ANY TAKES ~15s FOR 48x48 GRID\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _calc_pnet_consecutive_numba(pr_data, cat_data, acc_thresh=0.2):\n",
    "    \"\"\"Numba-optimized function to calculate Pnet for consecutive rain days.\"\"\"\n",
    "    pnet = pr_data.copy()\n",
    "    n_time = len(pr_data)\n",
    "    \n",
    "    # Find consecutive rain day indices - more efficient approach\n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if cat_data[i] == 2:\n",
    "            # Found start of consecutive rain event\n",
    "            event_start = i\n",
    "            # Find end of consecutive event\n",
    "            while i < n_time and cat_data[i] == 2:\n",
    "                i += 1\n",
    "            event_end = i - 1\n",
    "            \n",
    "            # Process this consecutive rain event\n",
    "            accpr = np.float32(0)\n",
    "            thresh_flag = False\n",
    "            \n",
    "            for j in range(event_start, event_end + 1):\n",
    "                accpr += pr_data[j]\n",
    "                \n",
    "                if accpr <= acc_thresh and not thresh_flag:\n",
    "                    pnet[j] = np.float32(0)\n",
    "                elif accpr > acc_thresh and not thresh_flag:\n",
    "                    accpr -= acc_thresh\n",
    "                    pnet[j] = accpr\n",
    "                    thresh_flag = True\n",
    "                else:\n",
    "                    pnet[j] = pr_data[j]\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return pnet\n",
    "\n",
    "\n",
    "@jit(nopython=True,fastmath=True)\n",
    "def _calc_kbdi_timeseries_numba(T_data, pnet_data, mean_ann_pr, day_int):\n",
    "    \"\"\"Numba-optimized KBDI time series calculation.\"\"\"\n",
    "    n_time = len(T_data)\n",
    "    KBDI = np.full(n_time, np.float32(np.nan))\n",
    "    \n",
    "    if day_int >= 0 and day_int < n_time:\n",
    "        KBDI[day_int] = np.float32(0)\n",
    "        \n",
    "        # Pre-calculate constant denominator\n",
    "        denominator = np.float32(1 + 10.88 * np.exp(-0.0441 * mean_ann_pr))\n",
    "        inv_denominator = np.float32(1e-3) / denominator  # Pre-calculate division\n",
    "        \n",
    "        for it in range(day_int + 1, n_time):\n",
    "            Q = max(np.float32(0), KBDI[it-1] - pnet_data[it] *np.float32(100))\n",
    "            numerator = np.float32((800 - Q) * (0.968 * np.exp(0.0486 * T_data[it]) - 8.3))\n",
    "            KBDI[it] = Q + numerator * inv_denominator\n",
    "    \n",
    "    return KBDI\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _calculate_consecutive_rain_categories_numba(rainmask):\n",
    "    \"\"\"Optimized calculation of consecutive rain day categories.\"\"\"\n",
    "    n_time = len(rainmask)\n",
    "    cat_data = np.zeros(n_time, dtype=np.int8)\n",
    "    \n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if rainmask[i] > 0:\n",
    "            # Found start of rain event\n",
    "            event_start = i\n",
    "            event_length = 0\n",
    "            \n",
    "            # Count consecutive rain days\n",
    "            while i < n_time and rainmask[i] > 0:\n",
    "                event_length += 1\n",
    "                i += 1\n",
    "            \n",
    "            # Assign categories based on event length\n",
    "            if event_length == 1:\n",
    "                cat_data[event_start] = 1  # Single rain day\n",
    "            else:\n",
    "                # Multiple consecutive rain days\n",
    "                for j in range(event_start, event_start + event_length):\n",
    "                    cat_data[j] = 2\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return cat_data\n",
    "\n",
    "\n",
    "def calc_kbdi_vectorized_optimized(T, PR):\n",
    "    \"\"\"\n",
    "    Optimized vectorized KBDI calculation for multiple grid points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data with dimensions (time, lat, lon) in Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data with dimensions (time, lat, lon) in inches\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    # Create time index\n",
    "    time_index = np.arange(len(PR.time), dtype=np.int32)\n",
    "    PR = PR.assign_coords(time_index=('time', time_index))\n",
    "    T = T.assign_coords(time_index=('time', time_index))\n",
    "    \n",
    "    # Parameters\n",
    "    ndays = np.int8(7)\n",
    "    pr_thresh = np.float32(8.0)  # inches\n",
    "    acc_thresh = np.float32(0.2)  # inches\n",
    "    \n",
    "\n",
    "    # LAZY CALCULATIONS ON CHUNKED XARRAY (DASK) ARRAYS\n",
    "\n",
    "    # Calculate 7-day rolling precipitation sum - more efficient\n",
    "    pr_weeksum = PR.rolling(time=ndays, min_periods=ndays, center=False).sum('time')\n",
    "\n",
    "    # Calculate mean annual precipitation\n",
    "    mean_ann_pr = PR.groupby('time.year').sum(min_count=360).mean('year')\n",
    "    \n",
    "    # GRID-BY-GRID CALCULATIONS\n",
    "    # VECTORIZED AND PARALLELIZED WITH XARRAY APPLY_UFUNC AND DASK\n",
    "    # OPTIMIZED WITH NUMBA\n",
    "    \n",
    "    # Optimized saturation day finding\n",
    "    def find_first_saturation_day_optimized(pr_week_1d):\n",
    "        \"\"\"Optimized version using numpy operations.\"\"\"\n",
    "        valid_mask = ~np.isnan(pr_week_1d)\n",
    "        if not valid_mask.any():\n",
    "            return -1\n",
    "        \n",
    "        exceeds = pr_week_1d > pr_thresh\n",
    "        if not exceeds.any():\n",
    "            return -1\n",
    "        \n",
    "        return int(np.argmax(exceeds))\n",
    "    \n",
    "    # Apply across lat/lon dimensions\n",
    "    saturation_days = xr.apply_ufunc(\n",
    "        find_first_saturation_day_optimized,\n",
    "        pr_weeksum,\n",
    "        input_core_dims=[['time']],\n",
    "        output_dtypes=[np.int32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized')\n",
    "\n",
    "    # Define optimized function to process a single grid point\n",
    "    def process_single_point_optimized(pr_1d, t_1d, mean_ann_pr_val, sat_day):\n",
    "        \"\"\"Optimized processing for a single lat/lon point.\"\"\"\n",
    "        if np.isnan(mean_ann_pr_val) or sat_day < 0:\n",
    "            return np.full(len(pr_1d), np.float32(np.nan))\n",
    "\n",
    "        # Create rain mask (0 or 1) - more efficient\n",
    "        rainmask = (pr_1d > 0).astype(np.int8)\n",
    "        \n",
    "        # Calculate rainfall categories using optimized numba function\n",
    "        cat_1d = _calculate_consecutive_rain_categories_numba(rainmask)\n",
    "        \n",
    "        # Calculate Pnet for consecutive rain days\n",
    "        pnet_1d = _calc_pnet_consecutive_numba(\n",
    "            pr_1d, \n",
    "            cat_1d, \n",
    "            acc_thresh)\n",
    "        \n",
    "        # Apply single rain day adjustment - vectorized\n",
    "        single_mask = (cat_1d == 1)\n",
    "        pnet_1d = np.where(single_mask, np.maximum(np.float32(0), pnet_1d - acc_thresh), pnet_1d)\n",
    "        \n",
    "        # Calculate KBDI time series\n",
    "        kbdi_1d = _calc_kbdi_timeseries_numba(\n",
    "            t_1d,\n",
    "            pnet_1d,\n",
    "            mean_ann_pr_val,\n",
    "            sat_day)\n",
    "        \n",
    "        return kbdi_1d\n",
    "    \n",
    "    # Apply the function across all grid points\n",
    "    KBDI = xr.apply_ufunc(\n",
    "        process_single_point_optimized,\n",
    "        PR.swap_dims({'time': 'time_index'}),\n",
    "        T.swap_dims({'time': 'time_index'}),\n",
    "        mean_ann_pr,\n",
    "        saturation_days,\n",
    "        input_core_dims=[['time_index'], ['time_index'], [], []],\n",
    "        output_core_dims=[['time_index']],\n",
    "        output_dtypes=[np.float32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized'\n",
    "    )\n",
    "    \n",
    "    # Convert back to original time coordinate\n",
    "    KBDI = KBDI.swap_dims({'time_index': 'time'})\n",
    "    KBDI = KBDI.assign_coords(time=PR.time)\n",
    "\n",
    "    return KBDI\n",
    "\n",
    "\n",
    "def calc_kbdi_parallel_chunked_optimized(T, PR, chunks=None):\n",
    "    \"\"\"\n",
    "    Optimized parallel chunked version of KBDI calculation using Dask.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data with dimensions (time, lat, lon) - will be converted to Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data with dimensions (time, lat, lon) - will be converted to inches\n",
    "    chunks : dict, optional\n",
    "        Chunk sizes for dask arrays, e.g., {'lat': 10, 'lon': 10, 'time': -1}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    \n",
    "    if chunks is None:\n",
    "        # Optimized default chunking strategy\n",
    "        chunks = {'lat': min(32, T.sizes['lat']), \n",
    "                  'lon': min(32, T.sizes['lon']), \n",
    "                  'time': -1}  # Keep time together\n",
    "    \n",
    "    # Chunk the input data FIRST\n",
    "    # T_chunked = T.chunk(chunks)\n",
    "    # PR_chunked = PR.chunk(chunks)\n",
    "    T_chunked = T\n",
    "    PR_chunked = PR\n",
    "    \n",
    "    # Apply precision limit and unit conversions in parallel\n",
    "    T_chunked = T_chunked.round(2).astype('float32')  # round to 2 decimal places\n",
    "    T_chunked = T_chunked * np.float32(9/5) + np.float32(32.0)  # Convert to Fahrenheit \n",
    "    T_chunked.attrs = {'standard_name': 'air_temperature',\n",
    "                       'long_name': 'Temperature, daily maximum',\n",
    "                       'units': 'F'}\n",
    "\n",
    "    PR_chunked = PR_chunked.round(2).astype('float32')\n",
    "    PR_chunked = PR_chunked * np.float32(1/25.4)  # Convert to inches - optimized\n",
    "    PR_chunked.attrs = {'standard_name': 'precipitation', \n",
    "                        'long_name': 'Precipitation, daily total', \n",
    "                        'units': 'inches/day'}\n",
    "    \n",
    "    # Apply the optimized vectorized calculation\n",
    "    KBDI = calc_kbdi_vectorized_optimized(T_chunked, PR_chunked)\n",
    "    \n",
    "    return KBDI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54174326-dd06-438f-bf75-bba6f0046eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 45.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "\n",
    "year_start = '1951'\n",
    "year_end = '2024'\n",
    "# lat1, lat2 = 32, 34\n",
    "# lon1, lon2 = -90, -88\n",
    "\n",
    "# More aggressive chunking for better parallelization\n",
    "chunks = {'lat': 24, 'lon': 24, 'time': -1}  # Smaller chunks = more parallel tasks\n",
    "\n",
    "# Load data with optimized settings\n",
    "with xr.open_dataset(pr_file,chunks=chunks) as ds_pr:\n",
    "    pr = ds_pr.prcp.sel(time=slice(year_start,year_end))\n",
    "    # pr = ds_pr.prcp.sel(time=slice(year_start, year_end), \n",
    "    #                    lat=slice(lat1, lat2), \n",
    "    #                    lon=slice(lon1, lon2))\n",
    "\n",
    "with xr.open_dataset(tmax_file,chunks=chunks) as ds_tmax:\n",
    "    tmax = ds_tmax.tmax.sel(time=slice(year_start,year_end))\n",
    "    # tmax = ds_tmax.tmax.sel(time=slice(year_start, year_end), \n",
    "    #                        lat=slice(lat1, lat2), \n",
    "    #                        lon=slice(lon1, lon2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935eba70-289a-4306-8292-e585bc98538f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26907, 358, 753)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695fda1-b721-4350-87da-9c722e4e7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run optimized calculation\n",
    "kbdi_result = calc_kbdi_parallel_chunked_optimized(tmax, pr, chunks=chunks)\n",
    "kbdi_result = kbdi_result.compute()  # Trigger computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbde2ab-8f66-4bf5-91bc-6e1e1928083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_result = kbdi_result.sel(lat=32,lon=-90,method='nearest')\n",
    "KBDI = xr.open_dataset('KBDI.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743fa43-0822-401b-8d03-e0d7d662dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(claude_result-KBDI.__xarray_dataarray_variable__).plot(figsize=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c46e-801d-44df-a205-d42ae3c41996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:KBDI]",
   "language": "python",
   "name": "conda-env-KBDI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
