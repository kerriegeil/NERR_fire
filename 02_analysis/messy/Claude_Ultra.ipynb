{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2000ac-4467-411f-a487-216d570f4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "import dask\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80925a4f-0228-4a56-bbfa-59e4f3e70228",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, fastmath=True, cache=True)\n",
    "def _calc_pnet_consecutive_numba(pr_data, cat_data, acc_thresh=0.2):\n",
    "    \"\"\"Numba-optimized function to calculate Pnet for consecutive rain days.\"\"\"\n",
    "    pnet = pr_data.copy()\n",
    "    n_time = len(pr_data)\n",
    "    \n",
    "    # Find consecutive rain day indices - more efficient approach\n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if cat_data[i] == 2:\n",
    "            # Found start of consecutive rain event\n",
    "            event_start = i\n",
    "            # Find end of consecutive event\n",
    "            while i < n_time and cat_data[i] == 2:\n",
    "                i += 1\n",
    "            event_end = i - 1\n",
    "            \n",
    "            # Process this consecutive rain event\n",
    "            accpr = np.float32(0)\n",
    "            thresh_flag = False\n",
    "            \n",
    "            for j in range(event_start, event_end + 1):\n",
    "                accpr += pr_data[j]\n",
    "                \n",
    "                if accpr <= acc_thresh and not thresh_flag:\n",
    "                    pnet[j] = np.float32(0)\n",
    "                elif accpr > acc_thresh and not thresh_flag:\n",
    "                    accpr -= acc_thresh\n",
    "                    pnet[j] = accpr\n",
    "                    thresh_flag = True\n",
    "                else:\n",
    "                    pnet[j] = pr_data[j]\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return pnet\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True, cache=True)\n",
    "def _calc_kbdi_timeseries_numba(T_data, pnet_data, mean_ann_pr, day_int):\n",
    "    \"\"\"Numba-optimized KBDI time series calculation.\"\"\"\n",
    "    n_time = len(T_data)\n",
    "    KBDI = np.full(n_time, np.float32(np.nan))\n",
    "    \n",
    "    if day_int >= 0 and day_int < n_time:\n",
    "        KBDI[day_int] = np.float32(0)\n",
    "        \n",
    "        # Pre-calculate constant denominator\n",
    "        denominator = np.float32(1 + 10.88 * np.exp(-0.0441 * mean_ann_pr))\n",
    "        inv_denominator = np.float32(1e-3) / denominator  # Pre-calculate division\n",
    "        \n",
    "        for it in range(day_int + 1, n_time):\n",
    "            Q = max(np.float32(0), KBDI[it-1] - pnet_data[it] * np.float32(100))\n",
    "            numerator = np.float32((800 - Q) * (0.968 * np.exp(0.0486 * T_data[it]) - 8.3))\n",
    "            KBDI[it] = Q + numerator * inv_denominator\n",
    "    \n",
    "    return KBDI\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True, cache=True)\n",
    "def _calculate_consecutive_rain_categories_numba(rainmask):\n",
    "    \"\"\"Optimized calculation of consecutive rain day categories.\"\"\"\n",
    "    n_time = len(rainmask)\n",
    "    cat_data = np.zeros(n_time, dtype=np.int8)\n",
    "    \n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if rainmask[i] > 0:\n",
    "            # Found start of rain event\n",
    "            event_start = i\n",
    "            event_length = 0\n",
    "            \n",
    "            # Count consecutive rain days\n",
    "            while i < n_time and rainmask[i] > 0:\n",
    "                event_length += 1\n",
    "                i += 1\n",
    "            \n",
    "            # Assign categories based on event length\n",
    "            if event_length == 1:\n",
    "                cat_data[event_start] = 1  # Single rain day\n",
    "            else:\n",
    "                # Multiple consecutive rain days\n",
    "                for j in range(event_start, event_start + event_length):\n",
    "                    cat_data[j] = 2\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return cat_data\n",
    "\n",
    "\n",
    "def calc_kbdi_vectorized_optimized(T, PR):\n",
    "    \"\"\"\n",
    "    Optimized vectorized KBDI calculation for multiple grid points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data with dimensions (time, lat, lon) in Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data with dimensions (time, lat, lon) in inches\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    # Create time index\n",
    "    time_index = np.arange(len(PR.time), dtype=np.int32)\n",
    "    PR = PR.assign_coords(time_index=('time', time_index))\n",
    "    T = T.assign_coords(time_index=('time', time_index))\n",
    "    \n",
    "    # Parameters\n",
    "    ndays = np.int8(7)\n",
    "    pr_thresh = np.float32(8.0)  # inches\n",
    "    acc_thresh = np.float32(0.2)  # inches\n",
    "    \n",
    "    # LAZY CALCULATIONS ON CHUNKED XARRAY (DASK) ARRAYS\n",
    "    # Calculate 7-day rolling precipitation sum - more efficient\n",
    "    pr_weeksum = PR.rolling(time=ndays, min_periods=ndays, center=False).sum('time')\n",
    "\n",
    "    # Calculate mean annual precipitation - optimized groupby\n",
    "    mean_ann_pr = PR.resample(time='YE').sum(min_count=360).mean('time')\n",
    "    \n",
    "    # GRID-BY-GRID CALCULATIONS\n",
    "    # VECTORIZED AND PARALLELIZED WITH XARRAY APPLY_UFUNC AND DASK\n",
    "    # OPTIMIZED WITH NUMBA\n",
    "    \n",
    "    # Optimized saturation day finding\n",
    "    @jit(nopython=True, fastmath=True, cache=True)\n",
    "    def find_first_saturation_day_optimized(pr_week_1d, pr_thresh=8.0):\n",
    "        \"\"\"Optimized version using numba.\"\"\"\n",
    "        n_time = len(pr_week_1d)\n",
    "        for i in range(n_time):\n",
    "            if not np.isnan(pr_week_1d[i]) and pr_week_1d[i] > pr_thresh:\n",
    "                return np.int32(i)\n",
    "        return np.int32(-1)\n",
    "    \n",
    "    # Apply across lat/lon dimensions\n",
    "    saturation_days = xr.apply_ufunc(\n",
    "        find_first_saturation_day_optimized,\n",
    "        pr_weeksum,\n",
    "        input_core_dims=[['time']],\n",
    "        output_dtypes=[np.int32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized')\n",
    "\n",
    "    # Define optimized function to process a single grid point\n",
    "    @jit(nopython=True, fastmath=True, cache=True)\n",
    "    def process_single_point_optimized(pr_1d, t_1d, mean_ann_pr_val, sat_day, acc_thresh=0.2):\n",
    "        \"\"\"Optimized processing for a single lat/lon point.\"\"\"\n",
    "        if np.isnan(mean_ann_pr_val) or sat_day < 0:\n",
    "            return np.full(len(pr_1d), np.float32(np.nan))\n",
    "\n",
    "        # Create rain mask (0 or 1) - more efficient\n",
    "        n_time = len(pr_1d)\n",
    "        rainmask = np.zeros(n_time, dtype=np.int8)\n",
    "        for i in range(n_time):\n",
    "            if pr_1d[i] > 0:\n",
    "                rainmask[i] = 1\n",
    "        \n",
    "        # Calculate rainfall categories using optimized numba function\n",
    "        cat_1d = _calculate_consecutive_rain_categories_numba(rainmask)\n",
    "        \n",
    "        # Calculate Pnet for consecutive rain days\n",
    "        pnet_1d = _calc_pnet_consecutive_numba(pr_1d, cat_1d, acc_thresh)\n",
    "        \n",
    "        # Apply single rain day adjustment - vectorized\n",
    "        for i in range(n_time):\n",
    "            if cat_1d[i] == 1:\n",
    "                pnet_1d[i] = max(np.float32(0), pnet_1d[i] - acc_thresh)\n",
    "        \n",
    "        # Calculate KBDI time series\n",
    "        kbdi_1d = _calc_kbdi_timeseries_numba(t_1d, pnet_1d, mean_ann_pr_val, sat_day)\n",
    "        \n",
    "        return kbdi_1d\n",
    "    \n",
    "    # Apply the function across all grid points\n",
    "    KBDI = xr.apply_ufunc(\n",
    "        process_single_point_optimized,\n",
    "        PR.swap_dims({'time': 'time_index'}),\n",
    "        T.swap_dims({'time': 'time_index'}),\n",
    "        mean_ann_pr,\n",
    "        saturation_days,\n",
    "        input_core_dims=[['time_index'], ['time_index'], [], []],\n",
    "        output_core_dims=[['time_index']],\n",
    "        output_dtypes=[np.float32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized'\n",
    "    )\n",
    "    \n",
    "    # Convert back to original time coordinate\n",
    "    KBDI = KBDI.swap_dims({'time_index': 'time'})\n",
    "    KBDI = KBDI.assign_coords(time=PR.time)\n",
    "\n",
    "    return KBDI\n",
    "\n",
    "\n",
    "def calc_kbdi_parallel_chunked_optimized(T, PR, chunks=None):\n",
    "    \"\"\"\n",
    "    Optimized parallel chunked version of KBDI calculation using Dask.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data with dimensions (time, lat, lon) - will be converted to Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data with dimensions (time, lat, lon) - will be converted to inches\n",
    "    chunks : dict, optional\n",
    "        Chunk sizes for dask arrays, e.g., {'lat': 10, 'lon': 10, 'time': -1}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    \n",
    "    if chunks is None:\n",
    "        # More aggressive chunking for large datasets\n",
    "        lat_chunk = min(16, T.sizes['lat'])  # Smaller spatial chunks\n",
    "        lon_chunk = min(16, T.sizes['lon'])\n",
    "        chunks = {'lat': lat_chunk, 'lon': lon_chunk, 'time': -1}\n",
    "    \n",
    "    # Chunk the input data FIRST - smaller chunks for better parallelization\n",
    "    T_chunked = T.chunk(chunks)\n",
    "    PR_chunked = PR.chunk(chunks)\n",
    "    \n",
    "    # Apply precision limit and unit conversions in parallel\n",
    "    T_chunked = (T_chunked * np.float32(9/5) + np.float32(32.0)).astype('float32')  # Convert to Fahrenheit \n",
    "    T_chunked.attrs = {'standard_name': 'air_temperature',\n",
    "                       'long_name': 'Temperature, daily maximum',\n",
    "                       'units': 'F'}\n",
    "\n",
    "    PR_chunked = (PR_chunked * np.float32(1/25.4)).astype('float32')  # Convert to inches\n",
    "    PR_chunked.attrs = {'standard_name': 'precipitation', \n",
    "                        'long_name': 'Precipitation, daily total', \n",
    "                        'units': 'inches/day'}\n",
    "    \n",
    "    # Apply the optimized vectorized calculation\n",
    "    KBDI = calc_kbdi_vectorized_optimized(T_chunked, PR_chunked)\n",
    "    \n",
    "    return KBDI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ca3dc-1140-4e25-b148-188681f9cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "\n",
    "year_start = '1951'\n",
    "year_end = '2024'\n",
    "lat1, lat2 = 32, 34\n",
    "lon1, lon2 = -90, -88\n",
    "\n",
    "# Load data with optimized settings\n",
    "with xr.open_dataset(pr_file) as ds_pr:\n",
    "    pr = ds_pr.prcp.sel(time=slice(year_start, year_end), \n",
    "                       lat=slice(lat1, lat2), \n",
    "                       lon=slice(lon1, lon2))\n",
    "\n",
    "with xr.open_dataset(tmax_file) as ds_tmax:\n",
    "    tmax = ds_tmax.tmax.sel(time=slice(year_start, year_end), \n",
    "                           lat=slice(lat1, lat2), \n",
    "                           lon=slice(lon1, lon2))\n",
    "\n",
    "# More aggressive chunking for better parallelization\n",
    "chunks = {'lat': 24, 'lon': 24, 'time': -1}  # Smaller chunks = more parallel tasks\n",
    "\n",
    "# Run optimized calculation\n",
    "kbdi_result = calc_kbdi_parallel_chunked_optimized(tmax, pr, chunks=chunks)\n",
    "kbdi_result = kbdi_result.compute()  # Trigger computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbde2ab-8f66-4bf5-91bc-6e1e1928083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_result = kbdi_result.sel(lat=32,lon=-90,method='nearest')\n",
    "KBDI = xr.open_dataset('KBDI.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743fa43-0822-401b-8d03-e0d7d662dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(claude_result-KBDI.__xarray_dataarray_variable__).plot(figsize=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf28690-4022-44c0-8da6-9661bc17131a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da124e50-16c9-41d8-b728-d70414139822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:KBDI]",
   "language": "python",
   "name": "conda-env-KBDI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
