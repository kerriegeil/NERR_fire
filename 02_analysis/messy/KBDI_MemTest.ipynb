{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6b005-dcb4-46ae-b130-35e49319a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "import dask\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82398eca-d6c1-4cf4-a69e-ec16b2943c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is also optimized to run in the memory available on my laptop\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _calc_pnet_consecutive_numba(pr_data, cat_data, acc_thresh=0.2):\n",
    "    \"\"\"Numba-optimized function to calculate Pnet for consecutive rain days.\"\"\"\n",
    "    pnet = pr_data.copy()\n",
    "    n_time = len(pr_data)\n",
    "    \n",
    "    # Find consecutive rain day indices - more efficient approach\n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if cat_data[i] == 2:\n",
    "            # Found start of consecutive rain event\n",
    "            event_start = i\n",
    "            # Find end of consecutive event\n",
    "            while i < n_time and cat_data[i] == 2:\n",
    "                i += 1\n",
    "            event_end = i - 1\n",
    "            \n",
    "            # Process this consecutive rain event\n",
    "            accpr = np.float32(0)\n",
    "            thresh_flag = False\n",
    "            \n",
    "            for j in range(event_start, event_end + 1):\n",
    "                accpr += pr_data[j]\n",
    "                \n",
    "                if accpr <= acc_thresh and not thresh_flag:\n",
    "                    pnet[j] = np.float32(0)\n",
    "                elif accpr > acc_thresh and not thresh_flag:\n",
    "                    accpr -= acc_thresh\n",
    "                    pnet[j] = accpr\n",
    "                    thresh_flag = True\n",
    "                else:\n",
    "                    pnet[j] = pr_data[j]\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return pnet\n",
    "\n",
    "\n",
    "@jit(nopython=True,fastmath=True)\n",
    "def _calc_kbdi_timeseries_numba(T_data, pnet_data, mean_ann_pr, day_int):\n",
    "    \"\"\"Numba-optimized KBDI time series calculation.\"\"\"\n",
    "    n_time = len(T_data)\n",
    "    KBDI = np.full(n_time, np.float32(np.nan))\n",
    "    \n",
    "    if day_int >= 0 and day_int < n_time:\n",
    "        KBDI[day_int] = np.float32(0)\n",
    "        \n",
    "        # Pre-calculate constant denominator\n",
    "        denominator = np.float32(1 + 10.88 * np.exp(-0.0441 * mean_ann_pr))\n",
    "        inv_denominator = np.float32(1e-3) / denominator  # Pre-calculate division\n",
    "        \n",
    "        for it in range(day_int + 1, n_time):\n",
    "            Q = max(np.float32(0), KBDI[it-1] - pnet_data[it] *np.float32(100))\n",
    "            numerator = np.float32((800 - Q) * (0.968 * np.exp(0.0486 * T_data[it]) - 8.3))\n",
    "            KBDI[it] = Q + numerator * inv_denominator\n",
    "    \n",
    "    return KBDI\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _calculate_consecutive_rain_categories_numba(rainmask):\n",
    "    \"\"\"Optimized calculation of consecutive rain day categories.\"\"\"\n",
    "    n_time = len(rainmask)\n",
    "    cat_data = np.zeros(n_time, dtype=np.int8)\n",
    "    \n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if rainmask[i] > 0:\n",
    "            # Found start of rain event\n",
    "            event_start = i\n",
    "            event_length = 0\n",
    "            \n",
    "            # Count consecutive rain days\n",
    "            while i < n_time and rainmask[i] > 0:\n",
    "                event_length += 1\n",
    "                i += 1\n",
    "            \n",
    "            # Assign categories based on event length\n",
    "            if event_length == 1:\n",
    "                cat_data[event_start] = 1  # Single rain day\n",
    "            else:\n",
    "                # Multiple consecutive rain days\n",
    "                for j in range(event_start, event_start + event_length):\n",
    "                    cat_data[j] = 2\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return cat_data\n",
    "\n",
    "\n",
    "def calc_kbdi_vectorized_optimized(T, PR):\n",
    "    \"\"\"\n",
    "    Optimized vectorized KBDI calculation for multiple grid points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data with dimensions (time, lat, lon) in Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data with dimensions (time, lat, lon) in inches\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    # Create time index\n",
    "    time_index = np.arange(len(PR.time), dtype=np.int32)\n",
    "    PR = PR.assign_coords(time_index=('time', time_index))\n",
    "    T = T.assign_coords(time_index=('time', time_index))\n",
    "    \n",
    "    # Parameters\n",
    "    ndays = np.int8(7)\n",
    "    pr_thresh = np.float32(8.0)  # inches\n",
    "    acc_thresh = np.float32(0.2)  # inches\n",
    "    \n",
    "\n",
    "    # LAZY CALCULATIONS ON CHUNKED XARRAY (DASK) ARRAYS\n",
    "\n",
    "    # Calculate 7-day rolling precipitation sum - more efficient\n",
    "    pr_weeksum = PR.rolling(time=ndays, min_periods=ndays, center=False).sum('time')\n",
    "\n",
    "    # Calculate mean annual precipitation\n",
    "    mean_ann_pr = PR.groupby('time.year').sum(min_count=360).mean('year')\n",
    "    \n",
    "    # GRID-BY-GRID CALCULATIONS\n",
    "    # VECTORIZED AND PARALLELIZED WITH XARRAY APPLY_UFUNC AND DASK\n",
    "    # OPTIMIZED WITH NUMBA\n",
    "    \n",
    "    # Optimized saturation day finding\n",
    "    def find_first_saturation_day_optimized(pr_week_1d):\n",
    "        \"\"\"Optimized version using numpy operations.\"\"\"\n",
    "        valid_mask = ~np.isnan(pr_week_1d)\n",
    "        if not valid_mask.any():\n",
    "            return -1\n",
    "        \n",
    "        exceeds = pr_week_1d > pr_thresh\n",
    "        if not exceeds.any():\n",
    "            return -1\n",
    "        \n",
    "        return int(np.argmax(exceeds))\n",
    "    \n",
    "    # Apply across lat/lon dimensions\n",
    "    saturation_days = xr.apply_ufunc(\n",
    "        find_first_saturation_day_optimized,\n",
    "        pr_weeksum,\n",
    "        input_core_dims=[['time']],\n",
    "        output_dtypes=[np.int32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized')\n",
    "\n",
    "    # Define optimized function to process a single grid point\n",
    "    def process_single_point_optimized(pr_1d, t_1d, mean_ann_pr_val, sat_day):\n",
    "        \"\"\"Optimized processing for a single lat/lon point.\"\"\"\n",
    "        if np.isnan(mean_ann_pr_val) or sat_day < 0:\n",
    "            return np.full(len(pr_1d), np.float32(np.nan))\n",
    "\n",
    "        # Create rain mask (0 or 1)\n",
    "        rainmask = (pr_1d > 0).astype(np.int8)\n",
    "        \n",
    "        # Calculate rainfall categories using optimized numba function\n",
    "        cat_1d = _calculate_consecutive_rain_categories_numba(rainmask)\n",
    "        \n",
    "        # Calculate Pnet for consecutive rain days\n",
    "        pnet_1d = _calc_pnet_consecutive_numba(\n",
    "            pr_1d, \n",
    "            cat_1d, \n",
    "            acc_thresh)\n",
    "        \n",
    "        # Apply single rain day adjustment\n",
    "        single_mask = (cat_1d == 1)\n",
    "        pnet_1d = np.where(single_mask, np.maximum(np.float32(0), pnet_1d - acc_thresh), pnet_1d)\n",
    "        \n",
    "        # Calculate KBDI time series\n",
    "        kbdi_1d = _calc_kbdi_timeseries_numba(\n",
    "            t_1d,\n",
    "            pnet_1d,\n",
    "            mean_ann_pr_val,\n",
    "            sat_day)\n",
    "        \n",
    "        return kbdi_1d\n",
    "    \n",
    "    # Apply the function across all grid points\n",
    "    KBDI = xr.apply_ufunc(\n",
    "        process_single_point_optimized,\n",
    "        PR.swap_dims({'time': 'time_index'}),\n",
    "        T.swap_dims({'time': 'time_index'}),\n",
    "        mean_ann_pr,\n",
    "        saturation_days,\n",
    "        input_core_dims=[['time_index'], ['time_index'], [], []],\n",
    "        output_core_dims=[['time_index']],\n",
    "        output_dtypes=[np.float32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized'\n",
    "    )\n",
    "    \n",
    "    # Convert back to original time coordinate\n",
    "    KBDI = KBDI.swap_dims({'time_index': 'time'})\n",
    "    KBDI = KBDI.assign_coords(time=PR.time)\n",
    "\n",
    "    return KBDI\n",
    "\n",
    "\n",
    "def calc_kbdi_chunked_processing(T, PR, spatial_chunk_size=10):\n",
    "    \"\"\"\n",
    "    Process KBDI calculation in spatial chunks to minimize memory usage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data - will be converted to Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data - will be converted to inches\n",
    "    spatial_chunk_size : int\n",
    "        Size of spatial chunks for processing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get dimensions\n",
    "    n_lat, n_lon = T.sizes['lat'], T.sizes['lon']\n",
    "    \n",
    "    # Create output array template\n",
    "    kbdi_template = xr.zeros_like(T, dtype=np.float32)\n",
    "    kbdi_template = kbdi_template.rename('kbdi')\n",
    "    \n",
    "    # Process data in spatial chunks\n",
    "    lat_chunks = range(0, n_lat, spatial_chunk_size)\n",
    "    lon_chunks = range(0, n_lon, spatial_chunk_size)\n",
    "    \n",
    "    total_chunks = len(lat_chunks) * len(lon_chunks)\n",
    "    processed_chunks = 0\n",
    "    \n",
    "    print(f\"Processing {total_chunks} spatial chunks...\")\n",
    "    \n",
    "    # Store results for each chunk\n",
    "    kbdi_chunks = []\n",
    "    \n",
    "    for i, lat_start in enumerate(lat_chunks):\n",
    "        lat_end = min(lat_start + spatial_chunk_size, n_lat)\n",
    "        lat_slice = slice(lat_start, lat_end)\n",
    "        \n",
    "        lon_row_chunks = []\n",
    "        \n",
    "        for j, lon_start in enumerate(lon_chunks):\n",
    "            lon_end = min(lon_start + spatial_chunk_size, n_lon)\n",
    "            lon_slice = slice(lon_start, lon_end)\n",
    "            \n",
    "            print(f\"Processing chunk {processed_chunks + 1}/{total_chunks} \"\n",
    "                  f\"(lat: {lat_start}-{lat_end}, lon: {lon_start}-{lon_end})\")\n",
    "            \n",
    "            # Extract chunk data\n",
    "            T_chunk = T.isel(lat=lat_slice, lon=lon_slice).load()\n",
    "            PR_chunk = PR.isel(lat=lat_slice, lon=lon_slice).load()\n",
    "            \n",
    "            # Convert units\n",
    "            T_chunk = T_chunk.round(2).astype('float32')\n",
    "            T_chunk = T_chunk * np.float32(9/5) + np.float32(32.0)  # Convert to Fahrenheit\n",
    "            \n",
    "            PR_chunk = PR_chunk.round(2).astype('float32')\n",
    "            PR_chunk = PR_chunk * np.float32(1/25.4)  # Convert to inches\n",
    "            \n",
    "            # Process this chunk\n",
    "            kbdi_chunk = calc_kbdi_vectorized_optimized(T_chunk, PR_chunk)\n",
    "            \n",
    "            # Store result\n",
    "            lon_row_chunks.append(kbdi_chunk)\n",
    "            \n",
    "            # Clean up memory\n",
    "            del T_chunk, PR_chunk, kbdi_chunk\n",
    "            gc.collect()\n",
    "            \n",
    "            processed_chunks += 1\n",
    "        \n",
    "        # Concatenate longitude chunks for this latitude band\n",
    "        lat_band_result = xr.concat(lon_row_chunks, dim='lon')\n",
    "        kbdi_chunks.append(lat_band_result)\n",
    "        \n",
    "        # Clean up\n",
    "        del lon_row_chunks\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all latitude bands\n",
    "    print(\"Combining results...\")\n",
    "    kbdi_result = xr.concat(kbdi_chunks, dim='lat')\n",
    "    kbdi_result.name='kbdi'\n",
    "    \n",
    "    return kbdi_result\n",
    "\n",
    "\n",
    "def calc_kbdi_memory_efficient(pr_file, tmax_file, year_start='1951', year_end='2024', \n",
    "                              spatial_chunk_size=20):\n",
    "    \"\"\"\n",
    "    Memory-efficient KBDI calculation for large datasets.\n",
    "    Note: KBDI calculation is cumulative over time, so temporal chunking is not possible.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pr_file : str\n",
    "        Path to precipitation NetCDF file\n",
    "    tmax_file : str  \n",
    "        Path to temperature NetCDF file\n",
    "    year_start : str\n",
    "        Start year for processing\n",
    "    year_end : str\n",
    "        End year for processing\n",
    "    spatial_chunk_size : int\n",
    "        Size of spatial chunks for processing (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure Dask for memory efficiency\n",
    "    dask.config.set({\n",
    "        'array.chunk-size': '50MB',   # Smaller chunks for better memory control\n",
    "        'array.slicing.split_large_chunks': True,\n",
    "        'distributed.worker.memory.target': 0.6,  # Use 60% of available memory\n",
    "        'distributed.worker.memory.spill': 0.75,  # Spill to disk at 75%\n",
    "        'distributed.worker.memory.pause': 0.85   # Pause at 85%\n",
    "    })\n",
    "    \n",
    "    # Use smaller chunks for initial data loading - keep time dimension intact\n",
    "    load_chunks = {'lat': spatial_chunk_size, 'lon': spatial_chunk_size, 'time': -1}\n",
    "    \n",
    "    print(f\"Loading precipitation data...\")\n",
    "    with xr.open_dataset(pr_file, chunks=load_chunks) as ds_pr:\n",
    "        pr = ds_pr.prcp.sel(time=slice(year_start, year_end))\n",
    "        \n",
    "    print(f\"Loading temperature data...\")\n",
    "    with xr.open_dataset(tmax_file, chunks=load_chunks) as ds_tmax:\n",
    "        tmax = ds_tmax.tmax.sel(time=slice(year_start, year_end))\n",
    "    \n",
    "    print(f\"Data shape: {pr.shape}\")\n",
    "    print(f\"Processing with spatial chunks of size {spatial_chunk_size}x{spatial_chunk_size}\")\n",
    "    print(\"Note: Time dimension kept intact due to cumulative nature of KBDI calculation\")\n",
    "    \n",
    "    # Process only spatially chunked (time must remain intact)\n",
    "    kbdi_result = calc_kbdi_chunked_processing(tmax, pr, spatial_chunk_size)\n",
    "    \n",
    "    # Add metadata\n",
    "    kbdi_result.attrs = {\n",
    "        'standard_name': 'keetch_byram_drought_index',\n",
    "        'long_name': 'Keetch-Byram Drought Index',\n",
    "        'units': 'dimensionless'\n",
    "    }\n",
    "    \n",
    "    return kbdi_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64af75-701d-4419-a0dc-d881d41f2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Main execution with memory-efficient approach\n",
    "if __name__ == \"__main__\":\n",
    "    pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "    tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "    \n",
    "    year_start = '1951'\n",
    "    year_end = '2024'\n",
    "    \n",
    "    # Adjust spatial_chunk_size based on your memory constraints\n",
    "    # For 30GB RAM with your data size (26907, 358, 753):\n",
    "    # - spatial_chunk_size=20: ~3-4GB peak memory usage\n",
    "    # - spatial_chunk_size=15: ~2-3GB peak memory usage  \n",
    "    # - spatial_chunk_size=10: ~1-2GB peak memory usage\n",
    "    # - spatial_chunk_size=8:  ~1GB peak memory usage (most conservative)\n",
    "    \n",
    "    chunk_sizes_to_try = [128,64]  # Try progressively smaller chunks\n",
    "    \n",
    "    for chunk_size in chunk_sizes_to_try:\n",
    "        try:\n",
    "            print(f\"\\nAttempting with spatial_chunk_size={chunk_size}...\")\n",
    "            kbdi_result = calc_kbdi_memory_efficient(\n",
    "                pr_file, tmax_file, year_start, year_end, \n",
    "                spatial_chunk_size=chunk_size\n",
    "            )\n",
    "            \n",
    "            print(\"Calculation successful! Saving results...\")\n",
    "            # Save to NetCDF in chunks to avoid memory issues during writing\n",
    "            output_file = f'kbdi_nclimgrid_{year_start}-{year_end}.nc'\n",
    "            \n",
    "            # Use chunked encoding for efficient storage\n",
    "            # this part is broken\n",
    "            # encoding = {\n",
    "            #     kbdi_result.name: {\n",
    "            #         'zlib': True, \n",
    "            #         'complevel': 4,\n",
    "            #         'chunksizes': (min(365, len(kbdi_result.time)), \n",
    "            #                       min(chunk_size, len(kbdi_result.lat)), \n",
    "            #                       min(chunk_size, len(kbdi_result.lon)))\n",
    "            #     }\n",
    "            # }\n",
    "            \n",
    "            kbdi_result.to_netcdf(output_file)#, encoding=encoding)\n",
    "            print(f\"Results saved to {output_file}\")\n",
    "            print(f\"Final data shape: {kbdi_result.shape}\")\n",
    "            break  # Success - exit the retry loop\n",
    "            \n",
    "        except MemoryError as e:\n",
    "            print(f\"Memory error with chunk_size={chunk_size}: {e}\")\n",
    "            if chunk_size == chunk_sizes_to_try[-1]:  # Last attempt\n",
    "                print(\"ERROR: All chunk sizes failed. Consider:\")\n",
    "                print(\"1. Reducing the time range (process fewer years)\")\n",
    "                print(\"2. Processing subregions separately\") \n",
    "                print(\"3. Using a machine with more RAM\")\n",
    "                raise\n",
    "            else:\n",
    "                print(f\"Trying smaller chunk size...\")\n",
    "                # Force garbage collection before next attempt\n",
    "                gc.collect()\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21913e0-0c83-4b1d-86a6-3aa239b53202",
   "metadata": {},
   "outputs": [],
   "source": [
    "kbdi_result.to_netcdf(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f0164-e8b2-40d3-bfb4-4b101f6a576c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:KBDI]",
   "language": "python",
   "name": "conda-env-KBDI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
