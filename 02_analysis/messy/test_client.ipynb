{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189bb8a9-94d7-42c4-8dc5-b0280e2440ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c75318a-e420-4068-a56e-d33d6dd1f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def _calc_pnet_consecutive_numba(pr_data, cat_data, acc_thresh=0.2):\n",
    "    \"\"\"Numba-optimized function to calculate Pnet for consecutive rain days.\"\"\"\n",
    "    pnet = pr_data.copy()\n",
    "    n_time = len(pr_data)\n",
    "    \n",
    "    # Find consecutive rain day indices - more efficient approach\n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if cat_data[i] == 2:\n",
    "            # Found start of consecutive rain event\n",
    "            event_start = i\n",
    "            # Find end of consecutive event\n",
    "            while i < n_time and cat_data[i] == 2:\n",
    "                i += 1\n",
    "            event_end = i - 1\n",
    "            \n",
    "            # Process this consecutive rain event\n",
    "            accpr = np.float32(0)\n",
    "            thresh_flag = False\n",
    "            \n",
    "            for j in range(event_start, event_end + 1):\n",
    "                accpr += pr_data[j]\n",
    "                \n",
    "                if accpr <= acc_thresh and not thresh_flag:\n",
    "                    pnet[j] = np.float32(0)\n",
    "                elif accpr > acc_thresh and not thresh_flag:\n",
    "                    accpr -= acc_thresh\n",
    "                    pnet[j] = accpr\n",
    "                    thresh_flag = True\n",
    "                else:\n",
    "                    pnet[j] = pr_data[j]\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return pnet\n",
    "\n",
    "\n",
    "@jit(nopython=True,fastmath=True)\n",
    "def _calc_kbdi_timeseries_numba(T_data, pnet_data, mean_ann_pr, day_int):\n",
    "    \"\"\"Numba-optimized KBDI time series calculation.\"\"\"\n",
    "    n_time = len(T_data)\n",
    "    KBDI = np.full(n_time, np.float32(np.nan))\n",
    "    \n",
    "    if day_int >= 0 and day_int < n_time:\n",
    "        KBDI[day_int] = np.float32(0)\n",
    "        \n",
    "        # Pre-calculate constant denominator\n",
    "        denominator = np.float32(1 + 10.88 * np.exp(-0.0441 * mean_ann_pr))\n",
    "        inv_denominator = np.float32(1e-3) / denominator  # Pre-calculate division\n",
    "        \n",
    "        for it in range(day_int + 1, n_time):\n",
    "            Q = max(np.float32(0), KBDI[it-1] - pnet_data[it] *np.float32(100))\n",
    "            numerator = np.float32((800 - Q) * (0.968 * np.exp(0.0486 * T_data[it]) - 8.3))\n",
    "            KBDI[it] = Q + numerator * inv_denominator\n",
    "    \n",
    "    return KBDI\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _calculate_consecutive_rain_categories_numba(rainmask):\n",
    "    \"\"\"Optimized calculation of consecutive rain day categories.\"\"\"\n",
    "    n_time = len(rainmask)\n",
    "    cat_data = np.zeros(n_time, dtype=np.int8)\n",
    "    \n",
    "    i = 0\n",
    "    while i < n_time:\n",
    "        if rainmask[i] > 0:\n",
    "            # Found start of rain event\n",
    "            event_start = i\n",
    "            event_length = 0\n",
    "            \n",
    "            # Count consecutive rain days\n",
    "            while i < n_time and rainmask[i] > 0:\n",
    "                event_length += 1\n",
    "                i += 1\n",
    "            \n",
    "            # Assign categories based on event length\n",
    "            if event_length == 1:\n",
    "                cat_data[event_start] = 1  # Single rain day\n",
    "            else:\n",
    "                # Multiple consecutive rain days\n",
    "                for j in range(event_start, event_start + event_length):\n",
    "                    cat_data[j] = 2\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return cat_data\n",
    "\n",
    "\n",
    "@delayed  # ADD THIS DECORATOR!\n",
    "def process_chunk_delayed(pr_np, t_np, mean_ann_pr_np, sat_days_np):\n",
    "    \"\"\"\n",
    "    Delayed function to process a single spatial chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dimensions\n",
    "    n_time, n_lat, n_lon = pr_np.shape\n",
    "    \n",
    "    # Initialize output array\n",
    "    kbdi_chunk = np.full((n_time, n_lat, n_lon), np.float32(np.nan))\n",
    "    \n",
    "    # Process each grid point in this chunk\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            pr_1d = pr_np[:, i, j]\n",
    "            t_1d = t_np[:, i, j]\n",
    "            mean_ann_pr_val = mean_ann_pr_np[i, j]\n",
    "            sat_day = sat_days_np[i, j]\n",
    "            \n",
    "            # Skip if invalid data\n",
    "            if np.isnan(mean_ann_pr_val) or sat_day < 0:\n",
    "                continue\n",
    "                \n",
    "            # Parameters\n",
    "            acc_thresh = np.float32(0.2)\n",
    "            \n",
    "            # Create rain mask\n",
    "            rainmask = (pr_1d > 0).astype(np.int8)\n",
    "            \n",
    "            # Calculate rainfall categories\n",
    "            cat_1d = _calculate_consecutive_rain_categories_numba(rainmask)\n",
    "            \n",
    "            # Calculate Pnet for consecutive rain days\n",
    "            pnet_1d = _calc_pnet_consecutive_numba(pr_1d, cat_1d, acc_thresh)\n",
    "            \n",
    "            # Apply single rain day adjustment\n",
    "            single_mask = (cat_1d == 1)\n",
    "            pnet_1d = np.where(single_mask, np.maximum(np.float32(0), pnet_1d - acc_thresh), pnet_1d)\n",
    "            \n",
    "            # Calculate KBDI time series\n",
    "            kbdi_1d = _calc_kbdi_timeseries_numba(t_1d, pnet_1d, mean_ann_pr_val, sat_day)\n",
    "            \n",
    "            # Store result\n",
    "            kbdi_chunk[:, i, j] = kbdi_1d\n",
    "    \n",
    "    return kbdi_chunk\n",
    "\n",
    "\n",
    "def calc_kbdi_dask_delayed(T, PR, spatial_chunk_size=(20, 20)):\n",
    "    \"\"\"\n",
    "    Calculate KBDI using dask delayed for efficient parallel processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : xarray.DataArray\n",
    "        Temperature data with dimensions (time, lat, lon) in Fahrenheit\n",
    "    PR : xarray.DataArray  \n",
    "        Precipitation data with dimensions (time, lat, lon) in inches\n",
    "    spatial_chunk_size : tuple\n",
    "        Chunk sizes for lat and lon dimensions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values with same dimensions as input\n",
    "    \"\"\"\n",
    "    coords = PR.coords.copy()\n",
    "    \n",
    "    # Parameters\n",
    "    ndays = np.int8(7)\n",
    "    pr_thresh = np.float32(8.0)  # inches\n",
    "    \n",
    "    # Calculate 7-day rolling precipitation sum\n",
    "    print(\"Calculating 7-day precipitation sums...\")\n",
    "    pr_weeksum = PR.rolling(time=ndays, min_periods=ndays, center=False).sum('time')\n",
    "    \n",
    "    # Calculate mean annual precipitation\n",
    "    print(\"Calculating mean annual precipitation...\")\n",
    "    mean_ann_pr = PR.groupby('time.year').sum(min_count=360).mean('year')\n",
    "    \n",
    "    # Find saturation days\n",
    "    print(\"Finding saturation days...\")\n",
    "    def find_first_saturation_day_optimized(pr_week_1d):\n",
    "        \"\"\"Optimized version using numpy operations.\"\"\"\n",
    "        valid_mask = ~np.isnan(pr_week_1d)\n",
    "        if not valid_mask.any():\n",
    "            return -1\n",
    "        \n",
    "        exceeds = pr_week_1d > pr_thresh\n",
    "        if not exceeds.any():\n",
    "            return -1\n",
    "        \n",
    "        return int(np.argmax(exceeds))\n",
    "    \n",
    "    saturation_days = xr.apply_ufunc(\n",
    "        find_first_saturation_day_optimized,\n",
    "        pr_weeksum,\n",
    "        input_core_dims=[['time']],\n",
    "        output_dtypes=[np.int32],\n",
    "        vectorize=True,\n",
    "        dask='parallelized')\n",
    "    \n",
    "    mean_ann_pr_chunked = mean_ann_pr.chunk({'lat': spatial_chunk_size[0], 'lon': spatial_chunk_size[1]})\n",
    "    saturation_days_chunked = saturation_days.chunk({'lat': spatial_chunk_size[0], 'lon': spatial_chunk_size[1]})\n",
    "    \n",
    "    # Convert to dask delayed objects\n",
    "    print(\"Converting to delayed objects...\")\n",
    "    pr_delayed = PR.data.to_delayed().ravel()\n",
    "    t_delayed = T.data.to_delayed().ravel()\n",
    "    mean_ann_pr_delayed = mean_ann_pr_chunked.data.to_delayed().ravel()\n",
    "    sat_days_delayed = saturation_days_chunked.data.to_delayed().ravel()\n",
    "    print(len(pr_delayed),len(t_delayed),len(mean_ann_pr_delayed),len(sat_days_delayed))\n",
    "    \n",
    "    # Create delayed computation tasks\n",
    "    print(f\"Creating {len(pr_delayed)} delayed computation tasks...\")\n",
    "    zipvars = zip(pr_delayed, t_delayed, mean_ann_pr_delayed, sat_days_delayed)\n",
    "    task_list = [process_chunk_delayed(pr_chunk, t_chunk, mean_pr_chunk, sat_chunk) \n",
    "                for pr_chunk, t_chunk, mean_pr_chunk, sat_chunk in zipvars]\n",
    "    \n",
    "    # Compute all chunks in parallel\n",
    "    print(\"Computing all chunks in parallel...\")\n",
    "    kbdi_chunks = dask.compute(*task_list)\n",
    "    print(f'nchunks = {len(kbdi_chunks)}, chunk shape = {kbdi_chunks[0].shape}')\n",
    "    \n",
    "    print(\"Reconstructing full array...\")\n",
    "    # Create dask array from chunks and then reconstruct properly\n",
    "    kbdi_da = da.from_delayed(\n",
    "        dask.delayed(np.concatenate)(kbdi_chunks, axis=1), \n",
    "        shape=T.shape, \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Convert back to xarray with proper coordinates\n",
    "    KBDI = xr.DataArray(kbdi_da.compute(), name='kbdi', coords=coords)\n",
    "    \n",
    "    return KBDI\n",
    "\n",
    "\n",
    "def calc_kbdi_dask_optimized(pr_file, tmax_file, year_start='1951', year_end='2024', \n",
    "                            spatial_chunk_size=(20, 20), memory_limit='1.3GB', n_workers=20):\n",
    "    \"\"\"\n",
    "    Memory-efficient KBDI calculation using dask delayed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pr_file : str\n",
    "        Path to precipitation NetCDF file\n",
    "    tmax_file : str  \n",
    "        Path to temperature NetCDF file\n",
    "    year_start : str\n",
    "        Start year for processing\n",
    "    year_end : str\n",
    "        End year for processing\n",
    "    spatial_chunk_size : tuple\n",
    "        Chunk sizes for (lat, lon) dimensions\n",
    "    memory_limit : str\n",
    "        Memory limit for dask workers\n",
    "    n_workers : int\n",
    "        Number of worker threads to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    KBDI : xarray.DataArray\n",
    "        KBDI values\n",
    "    \"\"\"\n",
    "    \n",
    "    # SET UP DASK CLIENT FOR PARALLEL PROCESSING\n",
    "    print(f\"Setting up Dask client with {n_workers} workers...\")\n",
    "    client = Client(\n",
    "        processes=False,  # Use threads instead of processes for better memory sharing\n",
    "        n_workers=1,      # Single worker process\n",
    "        threads_per_worker=n_workers,  # Use all your CPU threads\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "    print(f\"Dask client dashboard: {client.dashboard_link}\")\n",
    "    \n",
    "    # Configure Dask for optimal performance with your RAM\n",
    "    dask.config.set({\n",
    "        'array.chunk-size': '100MB',\n",
    "        'array.slicing.split_large_chunks': True,\n",
    "        'distributed.worker.memory.target': 0.90,\n",
    "        'distributed.worker.memory.spill': 0.93,\n",
    "        'distributed.worker.memory.pause': 0.95,\n",
    "        'distributed.worker.memory.terminate': 0.98\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Load data with chunking - keep time intact but chunk spatially\n",
    "        load_chunks = {\n",
    "            'time': -1,  # Keep entire time series\n",
    "            'lat': spatial_chunk_size[0], \n",
    "            'lon': spatial_chunk_size[1]\n",
    "        }\n",
    "        \n",
    "        print(f\"Loading precipitation data with chunks: {load_chunks}\")\n",
    "        pr = xr.open_dataset(pr_file, chunks=load_chunks).prcp.sel(time=slice(year_start, year_end))\n",
    "\n",
    "            \n",
    "        print(f\"Loading temperature data with chunks: {load_chunks}\")\n",
    "        tmax = xr.open_dataset(tmax_file, chunks=load_chunks).tmax.sel(time=slice(year_start, year_end))\n",
    "\n",
    "        # Convert units efficiently\n",
    "        print(\"Converting units...\")\n",
    "        tmax = tmax * np.float32(9/5) + np.float32(32.0)  # Convert to Fahrenheit\n",
    "        tmax.attrs = {'standard_name': 'air_temperature',\n",
    "                      'long_name': 'Temperature, daily maximum',\n",
    "                      'units': 'F'}\n",
    "\n",
    "        pr = pr * np.float32(1/25.4)  # Convert to inches\n",
    "        pr.attrs = {'standard_name': 'precipitation', \n",
    "                    'long_name': 'Precipitation, daily total', \n",
    "                    'units': 'inches/day'}\n",
    "        \n",
    "        # Calculate KBDI using dask delayed\n",
    "        kbdi_result = calc_kbdi_dask_delayed(tmax, pr, spatial_chunk_size)\n",
    "        \n",
    "        # Add metadata\n",
    "        kbdi_result.attrs = {\n",
    "            'standard_name': 'keetch_byram_drought_index',\n",
    "            'long_name': 'Keetch-Byram Drought Index',\n",
    "            'units': 'dimensionless'\n",
    "        }\n",
    "        \n",
    "        return kbdi_result\n",
    "        \n",
    "    finally:\n",
    "        # Always close the client when done\n",
    "        client.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cde77e-210e-4273-bd0b-9f120dd0c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Attempting with spatial chunk size: (-1, 5)\n",
      "============================================================\n",
      "Setting up Dask client with 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerrie\\anaconda3\\envs\\KBDI\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 54451 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client dashboard: http://192.208.139.107:54451/status\n",
      "Loading precipitation data with chunks: {'time': -1, 'lat': -1, 'lon': 5}\n",
      "Loading temperature data with chunks: {'time': -1, 'lat': -1, 'lon': 5}\n",
      "Converting units...\n",
      "Calculating 7-day precipitation sums...\n",
      "Calculating mean annual precipitation...\n",
      "Finding saturation days...\n",
      "Converting to delayed objects...\n",
      "151 151 151 151\n",
      "Creating 151 delayed computation tasks...\n",
      "Computing all chunks in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kerrie\\anaconda3\\envs\\KBDI\\Lib\\site-packages\\distributed\\client.py:3370: UserWarning: Sending large graph of size 34.54 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2025-06-03 14:41:17,801 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 885.80 MiB -- Worker memory limit: 1.21 GiB\n",
      "2025-06-03 14:41:21,032 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 0.98 GiB -- Worker memory limit: 1.21 GiB\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pr_file = r'D://data/nclimgrid_daily/prcp_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "    tmax_file = r'D://data/nclimgrid_daily/tmax_nClimGridDaily_1951-2024_USsouth.nc'\n",
    "    \n",
    "    year_start = '1951'\n",
    "    year_end = '2024'\n",
    "    \n",
    "    # Try different chunk sizes - start with larger chunks for efficiency\n",
    "    chunk_sizes_to_try = [(-1, 5)]\n",
    "    \n",
    "    for chunk_size in chunk_sizes_to_try:\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Attempting with spatial chunk size: {chunk_size}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            kbdi_result = calc_kbdi_dask_optimized(\n",
    "                pr_file, tmax_file, year_start, year_end, \n",
    "                spatial_chunk_size=chunk_size,\n",
    "                n_workers=20  # USE ALL 20 THREADS\n",
    "            )\n",
    "            \n",
    "            print(\"\\nCalculation successful! Saving results...\")\n",
    "            output_file = f'kbdi_nclimgrid_{year_start}_{year_end}_dask.nc'\n",
    "\n",
    "            \n",
    "            kbdi_result.to_netcdf(output_file)\n",
    "            print(f\"Results saved to {output_file}\")\n",
    "            print(f\"Final data shape: {kbdi_result.shape}\")\n",
    "            print(f\"Success with chunk size: {chunk_size}\")\n",
    "            break\n",
    "            \n",
    "        except MemoryError as e:\n",
    "            print(f\"Memory error with chunk size {chunk_size}: {e}\")\n",
    "            if chunk_size == chunk_sizes_to_try[-1]:\n",
    "                print(\"ERROR: All chunk sizes failed!\")\n",
    "                raise\n",
    "            else:\n",
    "                print(\"Trying smaller chunk size...\")\n",
    "                gc.collect()\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with chunk size {chunk_size}: {e}\")\n",
    "            if chunk_size == chunk_sizes_to_try[-1]:\n",
    "                raise\n",
    "            else:\n",
    "                print(\"Trying smaller chunk size...\")\n",
    "                gc.collect()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eecda74-3ad0-45fd-9bbc-b6a9e4939813",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m client.shutdown()\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dee098-7aea-4134-8fda-3fc49f6cd93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:KBDI]",
   "language": "python",
   "name": "conda-env-KBDI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
